{"cells":[{"cell_type":"markdown","id":"45750e5c-5305-46e6-bd70-1774ae540fdc","metadata":{"id":"45750e5c-5305-46e6-bd70-1774ae540fdc"},"source":["# NOTEBOOK 3.6 Spark DataFrames"]},{"cell_type":"markdown","id":"393fc497-fc0d-4f34-b784-1c4d6a076424","metadata":{"id":"393fc497-fc0d-4f34-b784-1c4d6a076424"},"source":["## 0. Create a Spark session object"]},{"cell_type":"code","execution_count":null,"id":"3d7d5b30-9c64-4424-a2a9-81e70647ff73","metadata":{"id":"3d7d5b30-9c64-4424-a2a9-81e70647ff73","outputId":"a37c7c89-8d12-418e-a69b-59c882d82df6"},"outputs":[{"name":"stderr","output_type":"stream","text":["24/06/05 10:12:34 WARN Utils: Your hostname, PC25. resolves to a loopback address: 127.0.1.1; using 192.168.76.195 instead (on interface eth0)\n","24/06/05 10:12:34 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n","Setting default log level to \"WARN\".\n","To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n","24/06/05 10:12:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"]}],"source":["from pyspark.sql import SparkSession\n","\n","spark = SparkSession\\\n","        .builder\\\n","        .appName(\"DataFrameDemo\")\\\n","        .getOrCreate()"]},{"cell_type":"markdown","id":"da9894a8-60df-45cb-8918-34c6acf66a46","metadata":{"id":"da9894a8-60df-45cb-8918-34c6acf66a46"},"source":["## 1. File I/O with Spark DataFrame\n","\n","### 1.1 Load data from CSV file into a DataFrame\n","\n","Note:\n","- The Hadoop installation in the WSL distro has configured HDFS as the default file system.\n","- To access files in WSL's local file system, the filepath format will start with **\"file://**\".\n","- If files do not have header information in them, you can skip the (header, true) option.\n","\n","#### 1.1 Load data from a CSV file in HDFS"]},{"cell_type":"code","execution_count":null,"id":"2287bb65-dbbc-4f3d-84bb-fe21770c55f8","metadata":{"id":"2287bb65-dbbc-4f3d-84bb-fe21770c55f8","outputId":"04102ffe-0aca-490f-d49c-4aa6b99620a7"},"outputs":[{"name":"stdout","output_type":"stream","text":["+----+-----------+----------+--------+\n","|code|description|unit_price|quantity|\n","+----+-----------+----------+--------+\n","|1005|        pen|       2.5|       4|\n","|1007|     pencil|       1.0|      10|\n","|1001|   notebook|       5.0|       2|\n","+----+-----------+----------+--------+\n","only showing top 3 rows\n","\n"]}],"source":["sales_df = spark.read.option(\"sep\", \"\\t\")\\\n","    .option(\"header\", \"true\")\\\n","    .csv(\"data/sales.csv\")\n","\n","sales_df.show(3)"]},{"cell_type":"markdown","id":"fc4e2f64-36ee-47e4-8ddb-2b218658abc2","metadata":{"id":"fc4e2f64-36ee-47e4-8ddb-2b218658abc2"},"source":["### 1.2 Write/Read data from parquet file"]},{"cell_type":"code","execution_count":null,"id":"04360e3c-6303-432d-9c27-c868dd01a3ce","metadata":{"id":"04360e3c-6303-432d-9c27-c868dd01a3ce","outputId":"88fbd56e-f9a6-48f4-a9b2-ce4414d5e99a"},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["sales_df.write.parquet('data/sales.parquet')"]},{"cell_type":"code","execution_count":null,"id":"7f564550-e4aa-417e-abb5-f115bd4b0ea7","metadata":{"id":"7f564550-e4aa-417e-abb5-f115bd4b0ea7","outputId":"6422317a-5acc-4904-d1dd-828d44a4049c"},"outputs":[{"name":"stdout","output_type":"stream","text":["+----+-----------+----------+--------+\n","|code|description|unit_price|quantity|\n","+----+-----------+----------+--------+\n","|1005|        pen|       2.5|       4|\n","|1007|     pencil|       1.0|      10|\n","|1001|   notebook|       5.0|       2|\n","|1003|      ruler|       1.0|       1|\n","|1002| calculator|      55.0|       1|\n","+----+-----------+----------+--------+\n","\n"]}],"source":["parquet_df = spark.read.parquet(\"data/sales.parquet\")\n","parquet_df.show()"]},{"cell_type":"markdown","id":"20eac86f-dcbb-4224-98ba-8a81ff08dec3","metadata":{"id":"20eac86f-dcbb-4224-98ba-8a81ff08dec3"},"source":["#### Query"]},{"cell_type":"code","execution_count":null,"id":"daeacdc8-8c4b-4d04-9ca7-d562e9cfce4f","metadata":{"id":"daeacdc8-8c4b-4d04-9ca7-d562e9cfce4f","outputId":"19670446-260f-41d9-ed8b-2fe07c90b2ac"},"outputs":[{"name":"stdout","output_type":"stream","text":["+----+-----------+\n","|code|description|\n","+----+-----------+\n","|1005|        pen|\n","|1007|     pencil|\n","|1001|   notebook|\n","+----+-----------+\n","\n"]}],"source":["parquet_df.createOrReplaceTempView(\"parquetSales\")\n","results_df = spark.sql(\"SELECT code, description FROM parquetSales WHERE quantity >= 2 AND quantity <= 20\")\n","results_df.show()"]},{"cell_type":"markdown","id":"1be3afb3-8845-4a0d-a98f-68808db7b4ff","metadata":{"id":"1be3afb3-8845-4a0d-a98f-68808db7b4ff"},"source":["### 1.3 Write/Read data from JSON file"]},{"cell_type":"code","execution_count":null,"id":"0a21fa89-639d-4175-8684-3c842486a5ba","metadata":{"id":"0a21fa89-639d-4175-8684-3c842486a5ba"},"outputs":[],"source":["sales_df.write.json(\"data/sales.json\")"]},{"cell_type":"code","execution_count":null,"id":"bfaa8f48-7293-437f-9b24-d69d7d4fbeb5","metadata":{"id":"bfaa8f48-7293-437f-9b24-d69d7d4fbeb5","outputId":"59cdb5f7-07bb-4f53-fa6c-66281ae5c899"},"outputs":[{"name":"stdout","output_type":"stream","text":["+----+-----------+--------+----------+\n","|code|description|quantity|unit_price|\n","+----+-----------+--------+----------+\n","|1005|        pen|       4|       2.5|\n","|1007|     pencil|      10|       1.0|\n","|1001|   notebook|       2|       5.0|\n","|1003|      ruler|       1|       1.0|\n","|1002| calculator|       1|      55.0|\n","+----+-----------+--------+----------+\n","\n"]}],"source":["json_df = spark.read.json(\"data/sales.json\")\n","json_df.show()"]},{"cell_type":"markdown","id":"f04ee945-e5c7-4a5e-a9e3-e486c54af8a6","metadata":{"id":"f04ee945-e5c7-4a5e-a9e3-e486c54af8a6"},"source":["## 2. DataFrame Operations (Transformations)"]},{"cell_type":"markdown","id":"fe2b8dfa-082c-4f31-ad0e-6e30788d4bde","metadata":{"id":"fe2b8dfa-082c-4f31-ad0e-6e30788d4bde"},"source":["### 2(a) Print the schema in a tree format"]},{"cell_type":"code","execution_count":null,"id":"f23b734d-e8c8-4b4c-8ea3-28262004f19b","metadata":{"id":"f23b734d-e8c8-4b4c-8ea3-28262004f19b","outputId":"928c79e4-4bc5-4b13-9318-084167a72b42"},"outputs":[{"name":"stdout","output_type":"stream","text":["root\n"," |-- code: string (nullable = true)\n"," |-- description: string (nullable = true)\n"," |-- unit_price: string (nullable = true)\n"," |-- quantity: string (nullable = true)\n","\n"]}],"source":["sales_df.printSchema()"]},{"cell_type":"markdown","id":"435f8ca1-367b-4525-a5be-8a5c43c90e5e","metadata":{"id":"435f8ca1-367b-4525-a5be-8a5c43c90e5e"},"source":["### 2(b) Convert multiple column types"]},{"cell_type":"code","execution_count":null,"id":"6b710ff0-f863-46d9-92ef-f34d73ef487c","metadata":{"id":"6b710ff0-f863-46d9-92ef-f34d73ef487c","outputId":"27baeb83-cfff-43ee-9562-91d29346721c"},"outputs":[{"name":"stdout","output_type":"stream","text":["root\n"," |-- code: string (nullable = true)\n"," |-- description: string (nullable = true)\n"," |-- unit_price: double (nullable = true)\n"," |-- quantity: integer (nullable = true)\n","\n"]}],"source":["# Cast columns to appropriate types\n","\n","from pyspark.sql.functions import col\n","from pyspark.sql.types import DoubleType, IntegerType\n","\n","sales_df = sales_df.withColumn('unit_price', col('unit_price').cast(DoubleType())) \\\n","            .withColumn('quantity', col('quantity').cast(IntegerType()))\n","\n","sales_df.printSchema()"]},{"cell_type":"markdown","id":"1db1cfcd-bb18-47b6-a4d1-300849c1925d","metadata":{"id":"1db1cfcd-bb18-47b6-a4d1-300849c1925d"},"source":["### 2(c) Select a set of columns from a DataFrame"]},{"cell_type":"code","execution_count":null,"id":"45976cc7-97b0-4069-b86d-89893c23eb20","metadata":{"id":"45976cc7-97b0-4069-b86d-89893c23eb20","outputId":"48634f6e-68a9-4ca0-d4e4-09730440b300"},"outputs":[{"name":"stdout","output_type":"stream","text":["+-----------+\n","|description|\n","+-----------+\n","|        pen|\n","|     pencil|\n","|   notebook|\n","|      ruler|\n","| calculator|\n","+-----------+\n","\n"]}],"source":["sales_df.select(\"description\").show()"]},{"cell_type":"markdown","id":"2b75c7bc-7328-4067-b0aa-a2aa44a5f4cf","metadata":{"id":"2b75c7bc-7328-4067-b0aa-a2aa44a5f4cf"},"source":["### 2(d) Filter rows from a DataFrame based on certain conditions"]},{"cell_type":"code","execution_count":null,"id":"337fd653-b774-4934-8f8a-e2b3cff7cfc6","metadata":{"id":"337fd653-b774-4934-8f8a-e2b3cff7cfc6","outputId":"212b6f2e-f8c6-4cbc-986c-f22633c1bea8"},"outputs":[{"name":"stdout","output_type":"stream","text":["+----+-----------+----------+--------+\n","|code|description|unit_price|quantity|\n","+----+-----------+----------+--------+\n","|1005|        pen|       2.5|       4|\n","|1001|   notebook|       5.0|       2|\n","|1002| calculator|      55.0|       1|\n","+----+-----------+----------+--------+\n","\n"]}],"source":["sales_df.filter(sales_df['unit_price'] > 2.00).show()"]},{"cell_type":"markdown","id":"5f9eaf47-5ac2-42ac-bc36-52e63c52108b","metadata":{"id":"5f9eaf47-5ac2-42ac-bc36-52e63c52108b"},"source":["### 2(e) Group rows in a DataFrame based on a set of columns & apply aggregated functions (e.g., count(), avg()) on the grouped dataset."]},{"cell_type":"code","execution_count":null,"id":"1009142b-0e07-4774-8d8d-e0e16d8282fb","metadata":{"id":"1009142b-0e07-4774-8d8d-e0e16d8282fb","outputId":"dcf135c9-9746-47cf-f52c-e1d341ff52cc"},"outputs":[{"name":"stdout","output_type":"stream","text":["+----+-----+\n","|code|count|\n","+----+-----+\n","|1007|    1|\n","|1005|    1|\n","|1003|    1|\n","|1002|    1|\n","|1001|    1|\n","+----+-----+\n","\n"]}],"source":["sales_df.groupBy(\"code\").count().show()"]},{"cell_type":"code","execution_count":null,"id":"778bc04e-9746-4a7f-b407-9e2d3413d447","metadata":{"id":"778bc04e-9746-4a7f-b407-9e2d3413d447","outputId":"e8470266-bebb-4a94-8d94-90d7faf677b3"},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------+-----+\n","|quantity|count|\n","+--------+-----+\n","|       1|    2|\n","|       4|    1|\n","|      10|    1|\n","|       2|    1|\n","+--------+-----+\n","\n"]}],"source":["sales_df.groupBy(\"quantity\").count().show()"]},{"cell_type":"markdown","id":"a472a7c3-914c-4e32-b711-61f4a6feae57","metadata":{"id":"a472a7c3-914c-4e32-b711-61f4a6feae57"},"source":["## 3. SQL Statements on DataFrames\n","\n","### 3.1 Temporary Views\n","Temporary views enables developers us run SQL queries in a program, and get the result as a DataFrame.\n","\n","#### 3.1(a) Local temporary view on DataFrame"]},{"cell_type":"code","execution_count":null,"id":"823589ed-3fe7-4929-9d90-18af11d2f740","metadata":{"id":"823589ed-3fe7-4929-9d90-18af11d2f740","outputId":"a70997ab-81ee-4191-e35c-41fb9d3d091e"},"outputs":[{"name":"stdout","output_type":"stream","text":["+----+-----------+----------+--------+\n","|code|description|unit_price|quantity|\n","+----+-----------+----------+--------+\n","|1005|        pen|       2.5|       4|\n","|1007|     pencil|       1.0|      10|\n","|1001|   notebook|       5.0|       2|\n","|1003|      ruler|       1.0|       1|\n","|1002| calculator|      55.0|       1|\n","+----+-----------+----------+--------+\n","\n"]}],"source":["# Create a temporary view on a DataFrame\n","sales_df.createOrReplaceTempView(\"sales\")\n","sqlDF = spark.sql(\"SELECT * FROM sales\")\n","sqlDF.show()"]},{"cell_type":"markdown","id":"129b436c-81eb-4dea-a12e-a12f03f3c7d1","metadata":{"id":"129b436c-81eb-4dea-a12e-a12f03f3c7d1"},"source":["#### 3.1(b) Global temporary views on DataFrames\n","Temporary views only last for the session in which they are created. If we want to have views available across various sessions, we need to create Global Temporary Views. The view definition is stored in the default database, **global_temp**. Once a view is created, we need to use the fully qualified name to access it in a query."]},{"cell_type":"code","execution_count":null,"id":"9ce48fea-d1dd-424e-960e-0e5b09d6a335","metadata":{"id":"9ce48fea-d1dd-424e-960e-0e5b09d6a335","outputId":"a5a25316-8414-44fe-e8b2-3c2e27174836"},"outputs":[{"name":"stdout","output_type":"stream","text":["+----+-----------+----------+--------+\n","|code|description|unit_price|quantity|\n","+----+-----------+----------+--------+\n","|1005|        pen|       2.5|       4|\n","|1007|     pencil|       1.0|      10|\n","|1001|   notebook|       5.0|       2|\n","|1003|      ruler|       1.0|       1|\n","|1002| calculator|      55.0|       1|\n","+----+-----------+----------+--------+\n","\n","+----+-----------+----------+--------+\n","|code|description|unit_price|quantity|\n","+----+-----------+----------+--------+\n","|1005|        pen|       2.5|       4|\n","|1007|     pencil|       1.0|      10|\n","|1001|   notebook|       5.0|       2|\n","|1003|      ruler|       1.0|       1|\n","|1002| calculator|      55.0|       1|\n","+----+-----------+----------+--------+\n","\n"]}],"source":["sales_df.createGlobalTempView(\"sales\")\n","\n","# Global temporary view is tied to a system database `global_temp`\n","spark.sql(\"SELECT * FROM global_temp.sales\").show()\n","spark.newSession().sql(\"SELECT * FROM global_temp.sales\").show()"]},{"cell_type":"markdown","id":"412862ee-b022-4048-ad08-af2209a637bb","metadata":{"id":"412862ee-b022-4048-ad08-af2209a637bb"},"source":["### 3.2 Use SparkSQL to read the columns with correct data types"]},{"cell_type":"code","execution_count":null,"id":"e08057e3-be9e-442d-b712-e7b7f8657ea9","metadata":{"id":"e08057e3-be9e-442d-b712-e7b7f8657ea9","outputId":"7bdc8f16-96be-4df6-93e0-14d28a3e7b8a"},"outputs":[{"name":"stdout","output_type":"stream","text":["root\n"," |-- code: string (nullable = true)\n"," |-- description: string (nullable = true)\n"," |-- unit_price: string (nullable = true)\n"," |-- quantity: string (nullable = true)\n","\n","+----+-----------+----------+--------+\n","|code|description|unit_price|quantity|\n","+----+-----------+----------+--------+\n","|1005|        pen|       2.5|       4|\n","|1007|     pencil|       1.0|      10|\n","|1001|   notebook|       5.0|       2|\n","|1003|      ruler|       1.0|       1|\n","|1002| calculator|      55.0|       1|\n","+----+-----------+----------+--------+\n","\n"]}],"source":["sales_df2 = spark.read.parquet(\"data/sales.parquet\")\n","sales_df2.printSchema()\n","sales_df2.show()"]},{"cell_type":"code","execution_count":null,"id":"fb16b503-4331-4f30-b83d-e90b61ee75ef","metadata":{"id":"fb16b503-4331-4f30-b83d-e90b61ee75ef","outputId":"d9c4980b-c805-460e-8bb2-03dc4604e8ef"},"outputs":[{"name":"stdout","output_type":"stream","text":["root\n"," |-- code: string (nullable = true)\n"," |-- description: string (nullable = true)\n"," |-- unit_price: double (nullable = true)\n"," |-- quantity: integer (nullable = true)\n","\n","+----+-----------+----------+--------+\n","|code|description|unit_price|quantity|\n","+----+-----------+----------+--------+\n","|1005|        pen|       2.5|       4|\n","|1007|     pencil|       1.0|      10|\n","|1001|   notebook|       5.0|       2|\n","|1003|      ruler|       1.0|       1|\n","|1002| calculator|      55.0|       1|\n","+----+-----------+----------+--------+\n","\n"]}],"source":["sales_df2.createOrReplaceTempView('SalesData')\n","sales_df2 = spark.sql(\"SELECT code, description, DOUBLE(unit_price), INT(quantity) from SalesData\")\n","sales_df2.printSchema()\n","sales_df2.show()"]},{"cell_type":"markdown","id":"ad7cc61c-7219-48e8-89e6-243400a9d026","metadata":{"id":"ad7cc61c-7219-48e8-89e6-243400a9d026"},"source":["### 3.3 Joining DataFrames"]},{"cell_type":"markdown","id":"3f806100-4ead-45e6-bed4-59952b06b9a4","metadata":{"id":"3f806100-4ead-45e6-bed4-59952b06b9a4"},"source":["#### Check the sales data"]},{"cell_type":"code","execution_count":null,"id":"c28c3d85-655f-4c4f-829d-e854668d9274","metadata":{"id":"c28c3d85-655f-4c4f-829d-e854668d9274","outputId":"c832dbf9-aa2d-4df4-abae-ecec86f6310c"},"outputs":[{"name":"stdout","output_type":"stream","text":["root\n"," |-- code: string (nullable = true)\n"," |-- description: string (nullable = true)\n"," |-- unit_price: double (nullable = true)\n"," |-- quantity: integer (nullable = true)\n","\n","+----+-----------+----------+--------+\n","|code|description|unit_price|quantity|\n","+----+-----------+----------+--------+\n","|1005|        pen|       2.5|       4|\n","|1007|     pencil|       1.0|      10|\n","|1001|   notebook|       5.0|       2|\n","|1003|      ruler|       1.0|       1|\n","|1002| calculator|      55.0|       1|\n","+----+-----------+----------+--------+\n","\n"]}],"source":["sales_df.printSchema()\n","sales_df.show()"]},{"cell_type":"markdown","id":"b922bf37-b962-41ec-860f-e37ac33ab9b2","metadata":{"id":"b922bf37-b962-41ec-860f-e37ac33ab9b2"},"source":["#### Read the discount data"]},{"cell_type":"code","execution_count":null,"id":"9e16fa7b-c91a-4645-aef7-2e667ed5c59a","metadata":{"id":"9e16fa7b-c91a-4645-aef7-2e667ed5c59a","outputId":"82299268-fb81-4c19-9588-5ceb2c6d4909"},"outputs":[{"name":"stdout","output_type":"stream","text":["root\n"," |-- item_code: string (nullable = true)\n"," |-- discount_perc: double (nullable = true)\n","\n","+---------+-------------+\n","|item_code|discount_perc|\n","+---------+-------------+\n","|     1005|         20.0|\n","|     1007|         10.0|\n","|     1001|         50.0|\n","|     1003|         15.0|\n","|     1002|         10.0|\n","+---------+-------------+\n","\n"]}],"source":["discount_df = spark.read.option(\"sep\", \"\\t\")\\\n","    .option(\"header\", \"true\")\\\n","    .csv(\"data/discounts.csv\")\n","\n","discount_df.createOrReplaceTempView('DiscountData')\n","discount_df = spark.sql(\"SELECT item_code, DOUBLE(discount_perc) from DiscountData\")\n","discount_df.printSchema()\n","discount_df.show()"]},{"cell_type":"code","execution_count":null,"id":"81bb506e-5332-4c9b-a592-3683b3a8c271","metadata":{"id":"81bb506e-5332-4c9b-a592-3683b3a8c271"},"outputs":[],"source":["#### Join sales_df and discount_df based on the item code"]},{"cell_type":"code","execution_count":null,"id":"885a51c5-0d83-481f-a9bc-5f6bde5d39ff","metadata":{"id":"885a51c5-0d83-481f-a9bc-5f6bde5d39ff","outputId":"240efcc1-2ee9-4464-f93f-f71663ca1a33"},"outputs":[{"name":"stdout","output_type":"stream","text":["+----+-----------+----------+--------+---------+-------------+\n","|code|description|unit_price|quantity|item_code|discount_perc|\n","+----+-----------+----------+--------+---------+-------------+\n","|1005|        pen|       2.5|       4|     1005|         20.0|\n","|1007|     pencil|       1.0|      10|     1007|         10.0|\n","|1001|   notebook|       5.0|       2|     1001|         50.0|\n","|1003|      ruler|       1.0|       1|     1003|         15.0|\n","|1002| calculator|      55.0|       1|     1002|         10.0|\n","+----+-----------+----------+--------+---------+-------------+\n","\n"]}],"source":["sales_df = sales_df.join(discount_df, sales_df.code == discount_df.item_code, \"inner\")\n","sales_df.show()"]},{"cell_type":"markdown","id":"bcd411ec-39bd-414d-917d-a3d8941a22a4","metadata":{"id":"bcd411ec-39bd-414d-917d-a3d8941a22a4"},"source":["#### Drop item_code column"]},{"cell_type":"code","execution_count":null,"id":"305c337e-d9e9-48d5-b82e-93982d9e590b","metadata":{"id":"305c337e-d9e9-48d5-b82e-93982d9e590b","outputId":"14bd77cc-f6c7-4906-c199-a11b7b11c540"},"outputs":[{"name":"stdout","output_type":"stream","text":["+----+-----------+----------+--------+-------------+\n","|code|description|unit_price|quantity|discount_perc|\n","+----+-----------+----------+--------+-------------+\n","|1005|        pen|       2.5|       4|         20.0|\n","|1007|     pencil|       1.0|      10|         10.0|\n","|1001|   notebook|       5.0|       2|         50.0|\n","|1003|      ruler|       1.0|       1|         15.0|\n","|1002| calculator|      55.0|       1|         10.0|\n","+----+-----------+----------+--------+-------------+\n","\n"]}],"source":["sales_df = sales_df.drop('item_code')\n","sales_df.show()"]},{"cell_type":"markdown","id":"e16b973d-91c3-4068-99df-080b7c579c30","metadata":{"id":"e16b973d-91c3-4068-99df-080b7c579c30"},"source":["#### Rename code column"]},{"cell_type":"code","execution_count":null,"id":"6bfd7b44-8dec-4446-8762-3304bc2a4c7b","metadata":{"id":"6bfd7b44-8dec-4446-8762-3304bc2a4c7b","outputId":"3d4239ab-7ed2-416a-930e-b1d9b18cbedd"},"outputs":[{"name":"stdout","output_type":"stream","text":["+------------+-----------+----------+--------+-------------+\n","|product_code|description|unit_price|quantity|discount_perc|\n","+------------+-----------+----------+--------+-------------+\n","|        1005|        pen|       2.5|       4|         20.0|\n","|        1007|     pencil|       1.0|      10|         10.0|\n","|        1001|   notebook|       5.0|       2|         50.0|\n","|        1003|      ruler|       1.0|       1|         15.0|\n","|        1002| calculator|      55.0|       1|         10.0|\n","+------------+-----------+----------+--------+-------------+\n","\n"]}],"source":["sales_df = sales_df.withColumnRenamed('code', 'product_code')\n","sales_df.show()"]},{"cell_type":"markdown","id":"be83d67d-cba8-4474-9d56-bd870bd740c6","metadata":{"id":"be83d67d-cba8-4474-9d56-bd870bd740c6"},"source":["## 4. Convert Spark DataFrame to pandas DataFrame\n","**Note:** You have to install the **Pandas** package first.\n","\n","\n","PySpark DataFrame can be converted to a pandas DataFrame using the function toPandas()"]},{"cell_type":"code","execution_count":null,"id":"7fe5ff94-c247-451e-8b44-b701d71d6fdb","metadata":{"id":"7fe5ff94-c247-451e-8b44-b701d71d6fdb","outputId":"cc7467c3-9ae7-40ee-ea4d-9c71ccbb1dce"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>product_code</th>\n","      <th>description</th>\n","      <th>unit_price</th>\n","      <th>quantity</th>\n","      <th>discount_perc</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1005</td>\n","      <td>pen</td>\n","      <td>2.5</td>\n","      <td>4</td>\n","      <td>20.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1007</td>\n","      <td>pencil</td>\n","      <td>1.0</td>\n","      <td>10</td>\n","      <td>10.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1001</td>\n","      <td>notebook</td>\n","      <td>5.0</td>\n","      <td>2</td>\n","      <td>50.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1003</td>\n","      <td>ruler</td>\n","      <td>1.0</td>\n","      <td>1</td>\n","      <td>15.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1002</td>\n","      <td>calculator</td>\n","      <td>55.0</td>\n","      <td>1</td>\n","      <td>10.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  product_code description  unit_price  quantity  discount_perc\n","0         1005         pen         2.5         4           20.0\n","1         1007      pencil         1.0        10           10.0\n","2         1001    notebook         5.0         2           50.0\n","3         1003       ruler         1.0         1           15.0\n","4         1002  calculator        55.0         1           10.0"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["pandasDF = sales_df.toPandas()\n","pandasDF.head()"]},{"cell_type":"code","execution_count":null,"id":"8ca820dc-931b-4bed-9ade-bb8b75f7a694","metadata":{"id":"8ca820dc-931b-4bed-9ade-bb8b75f7a694"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"3d7238d4-7f78-45c7-92a6-d329bf68f62b","metadata":{"id":"3d7238d4-7f78-45c7-92a6-d329bf68f62b"},"source":["## 5. User-Defined Function (UDF)\n","\n","### 5(a) Registering an exsting function as a UDF"]},{"cell_type":"code","execution_count":null,"id":"fed3e0a1-8f3e-4fb1-ae0c-847918a5ef48","metadata":{"id":"fed3e0a1-8f3e-4fb1-ae0c-847918a5ef48"},"outputs":[],"source":["from pyspark.sql.functions import udf\n","\n","def calculate_price(unit_price, quantity):\n","    return unit_price * quantity\n","\n","# UDF registration\n","calculate_price_udf = udf(calculate_price, DoubleType())"]},{"cell_type":"markdown","id":"cfb87fd3-2198-4dcd-8234-adee4b185dbf","metadata":{"id":"cfb87fd3-2198-4dcd-8234-adee4b185dbf"},"source":["#### Compute total price before discount"]},{"cell_type":"code","execution_count":null,"id":"45766c06-d897-4dce-8952-b44554f3dc33","metadata":{"id":"45766c06-d897-4dce-8952-b44554f3dc33","outputId":"f0c0418f-de71-49f6-eaea-d7f0991a9305"},"outputs":[{"name":"stdout","output_type":"stream","text":["+------------+-----------+----------+--------+-------------+--------------+\n","|product_code|description|unit_price|quantity|discount_perc|original_total|\n","+------------+-----------+----------+--------+-------------+--------------+\n","|        1005|        pen|       2.5|       4|         20.0|          10.0|\n","|        1007|     pencil|       1.0|      10|         10.0|          10.0|\n","|        1001|   notebook|       5.0|       2|         50.0|          10.0|\n","|        1003|      ruler|       1.0|       1|         15.0|           1.0|\n","|        1002| calculator|      55.0|       1|         10.0|          55.0|\n","+------------+-----------+----------+--------+-------------+--------------+\n","\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["sales_df = sales_df.withColumn(\"original_total\", calculate_price_udf('unit_price', 'quantity'))\n","sales_df.show()"]},{"cell_type":"markdown","id":"2f4c5b1a-81c4-4044-a8f0-2e776c21569f","metadata":{"id":"2f4c5b1a-81c4-4044-a8f0-2e776c21569f"},"source":["### 5(b) Using a UDF created using annotations\n","Refer to the SalesProcessor class in sales_processor.py.\n","This class contains 2 UDFs that were created using the **@udf** annotation. Note that to use this approach, the methods must be static methods (as indicated with the **@staticmethod**) annotation).\n","\n","#### Add a file to be downloaded with the Spark job on every node."]},{"cell_type":"code","execution_count":null,"id":"ddc2def6-c358-4893-a166-291f91ce7e87","metadata":{"id":"ddc2def6-c358-4893-a166-291f91ce7e87"},"outputs":[],"source":["sc = spark.sparkContext\n","sc.addFile(\"de_classes/sales_processor.py\")\n","\n","from sales_processor import SalesProcessor"]},{"cell_type":"markdown","id":"60bb20b5-08ca-4156-94f7-3953b2d1cafa","metadata":{"id":"60bb20b5-08ca-4156-94f7-3953b2d1cafa"},"source":["#### Invoke UDF to compute the discounted price"]},{"cell_type":"code","execution_count":null,"id":"666f8544-7a38-43c3-a882-1b304707c78f","metadata":{"id":"666f8544-7a38-43c3-a882-1b304707c78f","outputId":"bca5f2e3-3827-4629-b9ce-76544e829b5f"},"outputs":[{"name":"stdout","output_type":"stream","text":["+------------+-----------+----------+--------+-------------+--------------+---------------------+\n","|product_code|description|unit_price|quantity|discount_perc|original_total|discounted_unit_price|\n","+------------+-----------+----------+--------+-------------+--------------+---------------------+\n","|        1005|        pen|       2.5|       4|         20.0|          10.0|                  2.0|\n","|        1007|     pencil|       1.0|      10|         10.0|          10.0|                  0.9|\n","|        1001|   notebook|       5.0|       2|         50.0|          10.0|                  2.5|\n","|        1003|      ruler|       1.0|       1|         15.0|           1.0|                 0.85|\n","|        1002| calculator|      55.0|       1|         10.0|          55.0|                 49.5|\n","+------------+-----------+----------+--------+-------------+--------------+---------------------+\n","\n"]}],"source":["sales_df = sales_df.withColumn(\"discounted_unit_price\", SalesProcessor.calculate_discounted_price('unit_price', 'discount_perc'))\n","sales_df.show()"]},{"cell_type":"markdown","id":"fcc764ed-fe0d-4ac9-9274-c37b879e2a02","metadata":{"id":"fcc764ed-fe0d-4ac9-9274-c37b879e2a02"},"source":["#### Compute the discounted total"]},{"cell_type":"code","execution_count":null,"id":"dd398f37-0113-4d77-a3ae-5b7c5e4030c1","metadata":{"id":"dd398f37-0113-4d77-a3ae-5b7c5e4030c1","outputId":"4203dbac-d237-4bf3-9e55-eed475082c65"},"outputs":[{"name":"stdout","output_type":"stream","text":["+------------+-----------+----------+--------+-------------+--------------+---------------------+----------------+\n","|product_code|description|unit_price|quantity|discount_perc|original_total|discounted_unit_price|discounted_total|\n","+------------+-----------+----------+--------+-------------+--------------+---------------------+----------------+\n","|        1005|        pen|       2.5|       4|         20.0|          10.0|                  2.0|             8.0|\n","|        1007|     pencil|       1.0|      10|         10.0|          10.0|                  0.9|             9.0|\n","|        1001|   notebook|       5.0|       2|         50.0|          10.0|                  2.5|             5.0|\n","|        1003|      ruler|       1.0|       1|         15.0|           1.0|                 0.85|            0.85|\n","|        1002| calculator|      55.0|       1|         10.0|          55.0|                 49.5|            49.5|\n","+------------+-----------+----------+--------+-------------+--------------+---------------------+----------------+\n","\n"]}],"source":["sales_df = sales_df.withColumn(\"discounted_total\", calculate_price_udf('discounted_unit_price', 'quantity'))\n","sales_df.show()"]},{"cell_type":"code","execution_count":null,"id":"2521b8c3-5f07-46e7-839f-d0e4baf1e8db","metadata":{"id":"2521b8c3-5f07-46e7-839f-d0e4baf1e8db"},"outputs":[],"source":["#### Select fewer columns for summary df"]},{"cell_type":"code","execution_count":null,"id":"d30c00b8-c373-42d6-9cc5-d7378f5d1af7","metadata":{"id":"d30c00b8-c373-42d6-9cc5-d7378f5d1af7","outputId":"32c0b298-59eb-4407-b4e3-72b6cc01ec8c"},"outputs":[{"name":"stdout","output_type":"stream","text":["+------------+-----------+----------+--------+-------------+----------------+\n","|product_code|description|unit_price|quantity|discount_perc|discounted_total|\n","+------------+-----------+----------+--------+-------------+----------------+\n","|        1005|        pen|       2.5|       4|         20.0|             8.0|\n","|        1007|     pencil|       1.0|      10|         10.0|             9.0|\n","|        1001|   notebook|       5.0|       2|         50.0|             5.0|\n","|        1003|      ruler|       1.0|       1|         15.0|            0.85|\n","|        1002| calculator|      55.0|       1|         10.0|            49.5|\n","+------------+-----------+----------+--------+-------------+----------------+\n","\n"]}],"source":["sales_summary_df = sales_df.select('product_code', 'description', 'unit_price', 'quantity', 'discount_perc', 'discounted_total')\n","sales_summary_df.show()"]},{"cell_type":"code","execution_count":null,"id":"4a7182dc-1c1f-4ea3-84df-e3f90dfafdac","metadata":{"id":"4a7182dc-1c1f-4ea3-84df-e3f90dfafdac","outputId":"a195e200-0066-4b77-b463-23a7fb76b61c"},"outputs":[{"name":"stdout","output_type":"stream","text":["+------------+-----------+----------+--------+-------------+----------------+\n","|product_code|description|unit_price|quantity|discount_perc|discounted_total|\n","+------------+-----------+----------+--------+-------------+----------------+\n","|        1005|        pen|    RM2.50|       4|         20.0|          RM8.00|\n","|        1007|     pencil|    RM1.00|      10|         10.0|          RM9.00|\n","|        1001|   notebook|    RM5.00|       2|         50.0|          RM5.00|\n","|        1003|      ruler|    RM1.00|       1|         15.0|          RM0.85|\n","|        1002| calculator|   RM55.00|       1|         10.0|         RM49.50|\n","+------------+-----------+----------+--------+-------------+----------------+\n","\n"]}],"source":["# Invoke another UDF to format the price\n","sales_summary_df = sales_summary_df.withColumn('unit_price', SalesProcessor.format_price('unit_price'))\\\n","            .withColumn('discounted_total', SalesProcessor.format_price('discounted_total'))\n","sales_summary_df.show()"]},{"cell_type":"markdown","id":"26ca0a3d-7f42-46b1-8575-2c6e0c332459","metadata":{"id":"26ca0a3d-7f42-46b1-8575-2c6e0c332459"},"source":["# 6. RDD vs DataFrame\n","\n","## Task: To compute the average quantity for each person\n","\n","### 6.1 Using RDD"]},{"cell_type":"code","execution_count":null,"id":"ee83ddbc-9cc4-4213-bea5-4f92bd0bfe11","metadata":{"id":"ee83ddbc-9cc4-4213-bea5-4f92bd0bfe11","outputId":"e33514aa-0926-4660-eb9b-4986d32a0334"},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"data":{"text/plain":["[('Brooke', 22.5), ('TD', 35.0), ('Jules', 30.0), ('Denny', 31.0)]"]},"execution_count":33,"metadata":{},"output_type":"execute_result"}],"source":["# Get the SparkContext object\n","sc = spark.sparkContext\n","\n","# Create an RDD of tuples (name, quantity)\n","dataRDD = sc.parallelize([(\"Brooke\", 20), (\"Denny\", 31), (\"Jules\", 30),(\"TD\", 35), (\"Brooke\", 25)])\n","\n","# Use map and reduceByKey transformations with their lambda expressions to aggregate and then compute average\n","quantityRDD = (dataRDD\n",".map(lambda x: (x[0], (x[1], 1)))\n",".reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",".map(lambda x: (x[0], x[1][0]/x[1][1])))\n","\n","quantityRDD.collect()"]},{"cell_type":"markdown","id":"6d1d91d7-689d-4098-8d3e-17a81c40e28e","metadata":{"id":"6d1d91d7-689d-4098-8d3e-17a81c40e28e"},"source":["### 6.2 Using DataFrame"]},{"cell_type":"code","execution_count":null,"id":"724b5a73-c212-4de0-b8c6-b5926d119fd5","metadata":{"id":"724b5a73-c212-4de0-b8c6-b5926d119fd5","outputId":"68a1345f-da16-4a3b-a7b4-76070f3b2c86"},"outputs":[{"name":"stdout","output_type":"stream","text":["+------+-------------+\n","|  name|avg(quantity)|\n","+------+-------------+\n","|Brooke|         22.5|\n","| Denny|         31.0|\n","| Jules|         30.0|\n","|    TD|         35.0|\n","+------+-------------+\n","\n"]}],"source":["from pyspark.sql.functions import avg\n","\n","# Create a DataFrame\n","data_df = spark.createDataFrame([(\"Brooke\", 20), (\"Denny\", 31), (\"Jules\", 30), (\"TD\", 35), (\"Brooke\", 25)], [\"name\", \"quantity\"])\n","\n","# Group the same names together, aggregate their ages, and compute an average\n","avg_df = data_df.groupBy(\"name\").agg(avg(\"quantity\"))\n","\n","# Show the results of the final execution\n","avg_df.show()"]},{"cell_type":"markdown","id":"5e2c6957-8a8e-4c28-93ea-66f9a5a7f15e","metadata":{"id":"5e2c6957-8a8e-4c28-93ea-66f9a5a7f15e"},"source":["## 7. Another Example\n","https://zacks.one/spark-tutorial/#The-DataFrame-API\n","\n","**Data**: The SF Fire Department data set.\n","\n","### 7.1 Inferring the schema"]},{"cell_type":"code","execution_count":null,"id":"e813fe37-e6e6-49f9-9835-4959b4f77bc7","metadata":{"id":"e813fe37-e6e6-49f9-9835-4959b4f77bc7","outputId":"91c84e57-1549-4785-a7bf-8886cd3fc216"},"outputs":[{"name":"stdout","output_type":"stream","text":["root\n"," |-- CallNumber: string (nullable = true)\n"," |-- UnitID: string (nullable = true)\n"," |-- IncidentNumber: string (nullable = true)\n"," |-- CallType: string (nullable = true)\n"," |-- CallDate: string (nullable = true)\n"," |-- WatchDate: string (nullable = true)\n"," |-- CallFinalDisposition: string (nullable = true)\n"," |-- AvailableDtTm: string (nullable = true)\n"," |-- Address: string (nullable = true)\n"," |-- City: string (nullable = true)\n"," |-- Zipcode: string (nullable = true)\n"," |-- Battalion: string (nullable = true)\n"," |-- StationArea: string (nullable = true)\n"," |-- Box: string (nullable = true)\n"," |-- OriginalPriority: string (nullable = true)\n"," |-- Priority: string (nullable = true)\n"," |-- FinalPriority: string (nullable = true)\n"," |-- ALSUnit: string (nullable = true)\n"," |-- CallTypeGroup: string (nullable = true)\n"," |-- NumAlarms: string (nullable = true)\n"," |-- UnitType: string (nullable = true)\n"," |-- UnitSequenceInCallDispatch: string (nullable = true)\n"," |-- FirePreventionDistrict: string (nullable = true)\n"," |-- SupervisorDistrict: string (nullable = true)\n"," |-- Neighborhood: string (nullable = true)\n"," |-- Location: string (nullable = true)\n"," |-- RowID: string (nullable = true)\n"," |-- Delay: string (nullable = true)\n","\n"]}],"source":["fire_df = (spark\n","    .read\n","    .option(\"samplingRatio\", 0.001)\n","    .option(\"header\", True)\n","    .csv(\"data/sf-fire-calls.csv\"))\n","\n","fire_df.printSchema()"]},{"cell_type":"markdown","id":"e4c453da-7d17-4b64-88a4-58d481e27377","metadata":{"id":"e4c453da-7d17-4b64-88a4-58d481e27377"},"source":["### 7.2 Save to HDFS as a parquet file"]},{"cell_type":"code","execution_count":null,"id":"819366f2-4ed5-4b64-a80c-8ad7dc90fe2a","metadata":{"id":"819366f2-4ed5-4b64-a80c-8ad7dc90fe2a","outputId":"3edb47ab-1538-4966-8134-69b88483ceb1"},"outputs":[{"name":"stderr","output_type":"stream","text":["24/06/05 10:13:50 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n","24/06/05 10:13:50 WARN MemoryManager: Total allocation exceeds 50.00% (520,880,128 bytes) of heap memory\n","Scaling row group sizes to 97.02% for 4 writers\n","24/06/05 10:13:50 WARN MemoryManager: Total allocation exceeds 50.00% (520,880,128 bytes) of heap memory\n","Scaling row group sizes to 77.62% for 5 writers\n","24/06/05 10:13:50 WARN MemoryManager: Total allocation exceeds 50.00% (520,880,128 bytes) of heap memory\n","Scaling row group sizes to 64.68% for 6 writers\n","24/06/05 10:13:50 WARN MemoryManager: Total allocation exceeds 50.00% (520,880,128 bytes) of heap memory\n","Scaling row group sizes to 55.44% for 7 writers\n","24/06/05 10:13:50 WARN MemoryManager: Total allocation exceeds 50.00% (520,880,128 bytes) of heap memory\n","Scaling row group sizes to 48.51% for 8 writers\n","24/06/05 10:13:50 WARN MemoryManager: Total allocation exceeds 50.00% (520,880,128 bytes) of heap memory\n","Scaling row group sizes to 43.12% for 9 writers\n","24/06/05 10:13:50 WARN MemoryManager: Total allocation exceeds 50.00% (520,880,128 bytes) of heap memory\n","Scaling row group sizes to 38.81% for 10 writers\n","24/06/05 10:13:50 WARN MemoryManager: Total allocation exceeds 50.00% (520,880,128 bytes) of heap memory\n","Scaling row group sizes to 35.28% for 11 writers\n","24/06/05 10:13:51 WARN MemoryManager: Total allocation exceeds 50.00% (520,880,128 bytes) of heap memory\n","Scaling row group sizes to 38.81% for 10 writers\n","24/06/05 10:13:51 WARN MemoryManager: Total allocation exceeds 50.00% (520,880,128 bytes) of heap memory\n","Scaling row group sizes to 43.12% for 9 writers\n","24/06/05 10:13:51 WARN MemoryManager: Total allocation exceeds 50.00% (520,880,128 bytes) of heap memory\n","Scaling row group sizes to 48.51% for 8 writers\n","24/06/05 10:13:51 WARN MemoryManager: Total allocation exceeds 50.00% (520,880,128 bytes) of heap memory\n","Scaling row group sizes to 55.44% for 7 writers\n","24/06/05 10:13:51 WARN MemoryManager: Total allocation exceeds 50.00% (520,880,128 bytes) of heap memory\n","Scaling row group sizes to 64.68% for 6 writers\n","24/06/05 10:13:51 WARN MemoryManager: Total allocation exceeds 50.00% (520,880,128 bytes) of heap memory\n","Scaling row group sizes to 77.62% for 5 writers\n","24/06/05 10:13:51 WARN MemoryManager: Total allocation exceeds 50.00% (520,880,128 bytes) of heap memory\n","Scaling row group sizes to 97.02% for 4 writers\n","                                                                                \r"]}],"source":["parquet_path = \"data/sf-fire-calls.parquet\"\n","fire_df.write.format(\"parquet\").save(parquet_path)"]},{"cell_type":"code","execution_count":null,"id":"4932be1f-149c-4212-9c44-a5fce7509aa5","metadata":{"id":"4932be1f-149c-4212-9c44-a5fce7509aa5","outputId":"8ab6f971-047d-45c2-c958-db5c2df20495"},"outputs":[{"name":"stdout","output_type":"stream","text":["+----------+------+--------------+----------------+----------+----------+--------------------+--------------------+--------------------+----+-------+---------+-----------+----+----------------+--------+-------------+-------+-------------+---------+--------------+--------------------------+----------------------+------------------+------------------+--------------------+-------------+---------+\n","|CallNumber|UnitID|IncidentNumber|        CallType|  CallDate| WatchDate|CallFinalDisposition|       AvailableDtTm|             Address|City|Zipcode|Battalion|StationArea| Box|OriginalPriority|Priority|FinalPriority|ALSUnit|CallTypeGroup|NumAlarms|      UnitType|UnitSequenceInCallDispatch|FirePreventionDistrict|SupervisorDistrict|      Neighborhood|            Location|        RowID|    Delay|\n","+----------+------+--------------+----------------+----------+----------+--------------------+--------------------+--------------------+----+-------+---------+-----------+----+----------------+--------+-------------+-------+-------------+---------+--------------+--------------------------+----------------------+------------------+------------------+--------------------+-------------+---------+\n","|   1850050|   M15|         55301|Medical Incident|07/03/2000|07/02/2000|               Other|07/03/2000 07:38:...|200 Block of SAN ...|  SF|  94127|      B09|         39|8554|               3|       3|            3|   true|         NULL|        1|         MEDIC|                         2|                     9|                 7|West of Twin Peaks|(37.7325253339799...|001850050-M15|5.0833335|\n","|   1850058|   T01|         55309|          Alarms|07/03/2000|07/02/2000|               Other|07/03/2000 07:05:...| 100 Block of 9TH ST|  SF|  94103|      B02|         36|2336|               3|       3|            3|   true|         NULL|        1|         TRUCK|                         3|                     2|                 6|   South of Market|(37.7751756623626...|001850058-T01|2.7666667|\n","|   1850065|   RC2|         55316|Medical Incident|07/03/2000|07/02/2000|               Other|07/03/2000 08:03:...|2000 Block of MIS...|  SF|  94110|      B02|         07|5236|               3|       3|            3|   true|         NULL|        1|RESCUE CAPTAIN|                         3|                     2|                 9|           Mission|(37.7642444942899...|001850065-RC2|      2.7|\n","|   1850070|   M03|         55321|Medical Incident|07/03/2000|07/03/2000|               Other|07/03/2000 09:01:...|    6TH ST/MARKET ST|  SF|  94102|      B03|         01|2248|               3|       3|            3|   true|         NULL|        1|         MEDIC|                         2|                     3|                 6|   South of Market|(37.7822305756448...|001850070-M03|2.8666666|\n","|   1850104|   E08|         55354|Medical Incident|07/03/2000|07/03/2000|               Other|07/03/2000 11:26:...|   4TH ST/BRANNAN ST|  SF|  94107|      B03|         08|2223|               3|       3|            3|  false|         NULL|        1|        ENGINE|                         2|                     3|                 6|   South of Market|(37.7783268314649...|001850104-E08|1.5166667|\n","+----------+------+--------------+----------------+----------+----------+--------------------+--------------------+--------------------+----+-------+---------+-----------+----+----------------+--------+-------------+-------+-------------+---------+--------------+--------------------------+----------------------+------------------+------------------+--------------------+-------------+---------+\n","only showing top 5 rows\n","\n"]}],"source":["fire_parquet = spark.read.parquet(parquet_path)\n","fire_parquet.show(5)"]},{"cell_type":"markdown","id":"d3dff5da-364a-46a5-b23c-8d36fce3d78a","metadata":{"id":"d3dff5da-364a-46a5-b23c-8d36fce3d78a"},"source":["### 7.3 Projections and Filters\n","\n","A **projection** is a way to return only the rows matching a certain relational condition by using filters. In Spark, projections are done with the select() method, while filters can be expressed using the filter() or where() method."]},{"cell_type":"code","execution_count":null,"id":"d38c9344-e1bc-4b83-b594-ab69dd4c9d5b","metadata":{"id":"d38c9344-e1bc-4b83-b594-ab69dd4c9d5b","outputId":"620e197d-3b59-4040-c180-b070bcd15347"},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------------+----------------------+--------------+\n","|IncidentNumber|AvailableDtTm         |CallType      |\n","+--------------+----------------------+--------------+\n","|100000        |11/30/2000 12:25:31 PM|Structure Fire|\n","|10000108      |01/01/2010 02:20:58 AM|Alarms        |\n","|10000145      |01/01/2010 03:07:02 AM|Alarms        |\n","|10000149      |01/01/2010 03:10:44 AM|Outside Fire  |\n","|10000178      |01/01/2010 04:36:35 AM|Structure Fire|\n","+--------------+----------------------+--------------+\n","only showing top 5 rows\n","\n"]}],"source":["from pyspark.sql.functions import *\n","\n","fire_parquet.select(\"IncidentNumber\", \"AvailableDtTm\", \"CallType\") \\\n","    .where(col(\"CallType\") != \"Medical Incident\") \\\n","    .orderBy(\"IncidentNumber\") \\\n","    .show(5, truncate=False)"]},{"cell_type":"markdown","id":"b5e48744-5663-4044-bbdc-4aa40bef6c70","metadata":{"id":"b5e48744-5663-4044-bbdc-4aa40bef6c70"},"source":["#### 7.3(a) Find the number of distinct CallTypes"]},{"cell_type":"code","execution_count":null,"id":"b9fbfab3-3bca-47e5-b115-27528676505e","metadata":{"id":"b9fbfab3-3bca-47e5-b115-27528676505e","outputId":"38675d07-e985-4f43-8397-81ce99a60ffa"},"outputs":[{"name":"stdout","output_type":"stream","text":["+-----------------+\n","|DistinctCallTypes|\n","+-----------------+\n","|               30|\n","+-----------------+\n","\n"]}],"source":["(fire_parquet\n","    .select(\"CallType\")\n","    .where(col(\"CallType\").isNotNull())\n","    .agg(countDistinct(\"CallType\").alias(\"DistinctCallTypes\"))\n","    .show())"]},{"cell_type":"markdown","id":"4d28354c-4724-4df2-b7fa-c017fd477f50","metadata":{"id":"4d28354c-4724-4df2-b7fa-c017fd477f50"},"source":["#### 7.3(b) List the distinct call types in the data set"]},{"cell_type":"code","execution_count":null,"id":"cca27119-a204-447d-ab44-05d658b0e2c2","metadata":{"id":"cca27119-a204-447d-ab44-05d658b0e2c2","outputId":"0e6f43cd-499b-4aa9-dcf5-dc112b425d12"},"outputs":[{"name":"stdout","output_type":"stream","text":["+-----------------------------+\n","|CallType                     |\n","+-----------------------------+\n","|Elevator / Escalator Rescue  |\n","|Aircraft Emergency           |\n","|Alarms                       |\n","|Odor (Strange / Unknown)     |\n","|Citizen Assist / Service Call|\n","|HazMat                       |\n","|Explosion                    |\n","|Oil Spill                    |\n","|Vehicle Fire                 |\n","|Suspicious Package           |\n","+-----------------------------+\n","only showing top 10 rows\n","\n"]}],"source":["# Filter for only distinct non-null CallTypes from all the rows\n","(fire_parquet\n","    .select(\"CallType\")\n","    .where(col(\"CallType\").isNotNull())\n","    .distinct()\n","    .show(10, False))"]},{"cell_type":"markdown","id":"c82ea5e2-b823-4bf4-af6e-b53d4e029781","metadata":{"id":"c82ea5e2-b823-4bf4-af6e-b53d4e029781"},"source":["### 7.4 Renaming Columns\n","\n","The original column names in the SF Fire Department data set had spaces in them. For example, the column name IncidentNumber was Incident Number. Spaces in column names can be problematic, especially when you want to write or save a DataFrame as a Parquet file (which prohibits this).\n","\n","#### 7.4(a) Renaming columns by specifying the desired column names in the schema with StructField"]},{"cell_type":"code","execution_count":null,"id":"2861456e-38f3-4a35-97d5-95db82e11402","metadata":{"id":"2861456e-38f3-4a35-97d5-95db82e11402","outputId":"b9de26d4-a71a-4fcd-c264-032b3138f730"},"outputs":[{"name":"stdout","output_type":"stream","text":["+----------+------+--------------+----------------+----------+----------+--------------------+--------------------+--------------------+----+-------+---------+-----------+----+----------------+--------+-------------+-------+-------------+---------+--------+--------------------------+----------------------+------------------+--------------------+--------------------+-------------+---------+\n","|CallNumber|UnitID|IncidentNumber|        CallType|  CallDate| WatchDate|CallFinalDisposition|       AvailableDtTm|             Address|City|Zipcode|Battalion|StationArea| Box|OriginalPriority|Priority|FinalPriority|ALSUnit|CallTypeGroup|NumAlarms|UnitType|UnitSequenceInCallDispatch|FirePreventionDistrict|SupervisorDistrict|        Neighborhood|            Location|        RowID|    Delay|\n","+----------+------+--------------+----------------+----------+----------+--------------------+--------------------+--------------------+----+-------+---------+-----------+----+----------------+--------+-------------+-------+-------------+---------+--------+--------------------------+----------------------+------------------+--------------------+--------------------+-------------+---------+\n","|  20110016|   T13|       2003235|  Structure Fire|01/11/2002|01/10/2002|               Other|01/11/2002 01:51:...|2000 Block of CAL...|  SF|  94109|      B04|         38|3362|               3|       3|            3|  false|         NULL|        1|   TRUCK|                         2|                     4|                 5|     Pacific Heights|(37.7895840679362...|020110016-T13|     2.95|\n","|  20110022|   M17|       2003241|Medical Incident|01/11/2002|01/10/2002|               Other|01/11/2002 03:01:...|0 Block of SILVER...|  SF|  94124|      B10|         42|6495|               3|       3|            3|   true|         NULL|        1|   MEDIC|                         1|                    10|                10|Bayview Hunters P...|(37.7337623673897...|020110022-M17|      4.7|\n","|  20110023|   M41|       2003242|Medical Incident|01/11/2002|01/10/2002|               Other|01/11/2002 02:39:...|MARKET ST/MCALLIS...|  SF|  94102|      B03|         01|1455|               3|       3|            3|   true|         NULL|        1|   MEDIC|                         2|                     3|                 6|          Tenderloin|(37.7811772186856...|020110023-M41|2.4333334|\n","+----------+------+--------------+----------------+----------+----------+--------------------+--------------------+--------------------+----+-------+---------+-----------+----+----------------+--------+-------------+-------+-------------+---------+--------+--------------------------+----------------------+------------------+--------------------+--------------------+-------------+---------+\n","only showing top 3 rows\n","\n"]}],"source":["from pyspark.sql.types import *\n","\n","fire_schema = StructType([StructField('CallNumber', IntegerType(), True),\n","                StructField('UnitID', StringType(), True),\n","                StructField('IncidentNumber', IntegerType(), True),\n","                StructField('CallType', StringType(), True),\n","                StructField('CallDate', StringType(), True),\n","                StructField('WatchDate', StringType(), True),\n","                StructField('CallFinalDisposition', StringType(), True),\n","                StructField('AvailableDtTm', StringType(), True),\n","                StructField('Address', StringType(), True),\n","                StructField('City', StringType(), True),\n","                StructField('Zipcode', IntegerType(), True),\n","                StructField('Battalion', StringType(), True),\n","                StructField('StationArea', StringType(), True),\n","                StructField('Box', StringType(), True),\n","                StructField('OriginalPriority', StringType(), True),\n","                StructField('Priority', StringType(), True),\n","                StructField('FinalPriority', IntegerType(), True),\n","                StructField('ALSUnit', BooleanType(), True),\n","                StructField('CallTypeGroup', StringType(), True),\n","                StructField('NumAlarms', IntegerType(), True),\n","                StructField('UnitType', StringType(), True),\n","                StructField('UnitSequenceInCallDispatch', IntegerType(), True),\n","                StructField('FirePreventionDistrict', StringType(), True),\n","                StructField('SupervisorDistrict', StringType(), True),\n","                StructField('Neighborhood', StringType(), True),\n","                StructField('Location', StringType(), True),\n","                StructField('RowID', StringType(), True),\n","                StructField('Delay', FloatType(), True)])\n","\n","fire_df = (spark\n"," .read\n"," .csv(\"data/sf-fire-calls.csv\", header=True, schema=fire_schema))\n","\n","fire_df.show(3)"]},{"cell_type":"markdown","id":"8437276d-49c7-4718-9055-11009f92666d","metadata":{"id":"8437276d-49c7-4718-9055-11009f92666d"},"source":["#### 7.4(b) Renaming columns using the withColumnRenamed() method"]},{"cell_type":"code","execution_count":null,"id":"48719cb8-31b6-4c76-b59a-7cd02b75e444","metadata":{"id":"48719cb8-31b6-4c76-b59a-7cd02b75e444","outputId":"897a7e9c-4178-4ad7-f2f9-0f1748d17b1a"},"outputs":[{"name":"stdout","output_type":"stream","text":["+---------------------+\n","|ResponseDelayedinMins|\n","+---------------------+\n","|7.2166667            |\n","|8.666667             |\n","|16.016666            |\n","|9.933333             |\n","|8.833333             |\n","+---------------------+\n","only showing top 5 rows\n","\n"]}],"source":["# Change the name of the Delay column to ResponseDelayedinMins\n","new_fire_parquet = fire_parquet.withColumnRenamed(\"Delay\",\n","                                                  \"ResponseDelayedinMins\")\n","# select ResponseDelayedinMins > 5 mins\n","(new_fire_parquet\n","    .select(\"ResponseDelayedinMins\")\n","    .where(col(\"ResponseDelayedinMins\") > 5)\n","    .show(5, False))"]},{"cell_type":"markdown","id":"00f74c6f-21ce-4090-bbc2-57e02d4d3960","metadata":{"id":"00f74c6f-21ce-4090-bbc2-57e02d4d3960"},"source":["As DataFrame transformations are immutable, when we rename a column using withColumnRenamed() we get a new DataFrame while retaining the original with the old column name."]},{"cell_type":"code","execution_count":null,"id":"dda7d7f1-d861-4505-ac81-5116dad2bdc7","metadata":{"id":"dda7d7f1-d861-4505-ac81-5116dad2bdc7","outputId":"76d86f98-50df-4b13-e7ae-134206d8cc66"},"outputs":[{"data":{"text/plain":["['CallNumber',\n"," 'UnitID',\n"," 'IncidentNumber',\n"," 'CallType',\n"," 'CallDate',\n"," 'WatchDate',\n"," 'CallFinalDisposition',\n"," 'AvailableDtTm',\n"," 'Address',\n"," 'City',\n"," 'Zipcode',\n"," 'Battalion',\n"," 'StationArea',\n"," 'Box',\n"," 'OriginalPriority',\n"," 'Priority',\n"," 'FinalPriority',\n"," 'ALSUnit',\n"," 'CallTypeGroup',\n"," 'NumAlarms',\n"," 'UnitType',\n"," 'UnitSequenceInCallDispatch',\n"," 'FirePreventionDistrict',\n"," 'SupervisorDistrict',\n"," 'Neighborhood',\n"," 'Location',\n"," 'RowID',\n"," 'Delay']"]},"execution_count":43,"metadata":{},"output_type":"execute_result"}],"source":["fire_parquet.columns"]},{"cell_type":"markdown","id":"fff6a2c4-f9c6-49a3-852d-f2b853643629","metadata":{"id":"fff6a2c4-f9c6-49a3-852d-f2b853643629"},"source":["### 7.5 Format Conversion\n","1. Convert the existing columns data type from string to a Spark-supported timestamp.\n","2. Use the new format specified in the format string \"MM/dd/yyyy\" or \"MM/dd/yyyy\n","hh:mm:ss a\" where appropriate.\n","3. Drop the old column and append the new one specified in the first argument to the withColumn() method.\n","4. Assign the new modified DataFrame to fire_ts_df."]},{"cell_type":"code","execution_count":null,"id":"290e00ca-9d98-45cf-9c39-55b14409fed2","metadata":{"id":"290e00ca-9d98-45cf-9c39-55b14409fed2","outputId":"07cf2654-987d-4d0d-dfd1-746217affdcf"},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------------------+-------------------+-------------------+\n","|IncidentDate       |OnWatchDate        |AvailableDtTS      |\n","+-------------------+-------------------+-------------------+\n","|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 01:51:44|\n","|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 03:01:18|\n","|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 02:39:50|\n","|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 04:16:46|\n","|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 06:01:58|\n","+-------------------+-------------------+-------------------+\n","only showing top 5 rows\n","\n"]}],"source":["fire_ts_df = (fire_df\n",".withColumn(\"IncidentDate\", to_timestamp(col(\"CallDate\"), \"MM/dd/yyyy\"))\n",".drop(\"CallDate\")\n",".withColumn(\"OnWatchDate\", to_timestamp(col(\"WatchDate\"), \"MM/dd/yyyy\"))\n",".drop(\"WatchDate\")\n",".withColumn(\"AvailableDtTS\", to_timestamp(col(\"AvailableDtTm\"),\n","\"MM/dd/yyyy hh:mm:ss a\"))\n",".drop(\"AvailableDtTm\"))\n","\n","# Select the converted columns\n","(fire_ts_df\n",".select(\"IncidentDate\", \"OnWatchDate\", \"AvailableDtTS\")\n",".show(5, False))\n"]},{"cell_type":"markdown","id":"8b6da777-2561-410e-99fb-303fd4e92b3e","metadata":{"id":"8b6da777-2561-410e-99fb-303fd4e92b3e"},"source":["#### month(), year(), and day()"]},{"cell_type":"code","execution_count":null,"id":"08512681-c64d-4023-b7c5-3a31882ee94a","metadata":{"id":"08512681-c64d-4023-b7c5-3a31882ee94a","outputId":"c19f6d3a-8dcb-4250-b7dd-40b3420432b4"},"outputs":[{"name":"stdout","output_type":"stream","text":["+------------------+\n","|year(IncidentDate)|\n","+------------------+\n","|              2000|\n","|              2001|\n","|              2002|\n","|              2003|\n","|              2004|\n","+------------------+\n","only showing top 5 rows\n","\n"]}],"source":["(fire_ts_df\n",".select(year('IncidentDate'))\n",".distinct()\n",".orderBy(year('IncidentDate'))\n",".show(5))"]},{"cell_type":"markdown","id":"3e559830-75b6-44e5-8674-225f6cd304a2","metadata":{"id":"3e559830-75b6-44e5-8674-225f6cd304a2"},"source":["### 7.6 Aggregations\n","**groupBy()**, **orderBy()**, and **count()**, offer the ability to aggregate by column names and then aggregate counts"]},{"cell_type":"code","execution_count":null,"id":"d7d394df-8f6f-4435-95b2-6473245bb148","metadata":{"id":"d7d394df-8f6f-4435-95b2-6473245bb148","outputId":"3e626f1d-dca9-4407-c70d-ae121c1636b2"},"outputs":[{"name":"stdout","output_type":"stream","text":["+-----------------------------+------+\n","|CallType                     |count |\n","+-----------------------------+------+\n","|Medical Incident             |113794|\n","|Structure Fire               |23319 |\n","|Alarms                       |19406 |\n","|Traffic Collision            |7013  |\n","|Citizen Assist / Service Call|2524  |\n","+-----------------------------+------+\n","only showing top 5 rows\n","\n"]}],"source":["(fire_ts_df\n",".select(\"CallType\")\n",".where(col(\"CallType\").isNotNull())\n",".groupBy(\"CallType\")\n",".count()\n",".orderBy(\"count\", ascending=False)\n",".show(n=5, truncate=False))"]},{"cell_type":"markdown","id":"e5ae06ef-7163-45ed-96e6-393c3d827703","metadata":{"id":"e5ae06ef-7163-45ed-96e6-393c3d827703"},"source":["### 7.7 Descriptive Statistical Methods\n","\n","DataFrame API provides descriptive statistical methods like min(), max(), sum(), and avg()."]},{"cell_type":"code","execution_count":null,"id":"838528ef-b67e-4abe-a82e-b0fe36a8763c","metadata":{"id":"838528ef-b67e-4abe-a82e-b0fe36a8763c","outputId":"93d0b979-529f-48d9-a2e4-b496bac16824"},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------------+-----------------+-----------+----------+\n","|sum(NumAlarms)|       avg(Delay)| min(Delay)|max(Delay)|\n","+--------------+-----------------+-----------+----------+\n","|        176170|3.892364154521585|0.016666668|   1844.55|\n","+--------------+-----------------+-----------+----------+\n","\n"]}],"source":["import pyspark.sql.functions as F\n","(fire_ts_df\n",".select(F.sum(\"NumAlarms\"), F.avg(\"Delay\"), F.min(\"Delay\"), F.max(\"Delay\"))\n",".show())"]},{"cell_type":"markdown","source":["# 8. Join Operations\n","https://pedropark99.github.io/Introd-pyspark/Chapters/08-transforming2.html"],"metadata":{"id":"_oqJ8zd12Vnv"},"id":"_oqJ8zd12Vnv"},{"cell_type":"code","source":["# Create sample data\n","\n","info = [\n","    ('Mick', 'Rolling Stones', '1943-07-26', True),\n","    ('John', 'Beatles', '1940-09-10', True),\n","    ('Paul', 'Beatles', '1942-06-18', True),\n","    ('George', 'Beatles', '1943-02-25', True),\n","    ('Ringo', 'Beatles', '1940-07-07', True)\n","]\n","\n","info = spark.createDataFrame(\n","    info,\n","    ['name', 'band', 'born', 'children']\n",")\n","\n","band_instruments = [\n","    ('John', 'guitar'),\n","    ('Paul', 'bass'),\n","    ('Keith', 'guitar')\n","]\n","\n","band_instruments = spark.createDataFrame(\n","    band_instruments,\n","    ['name', 'plays']\n",")\n","\n","print(\"info:\")\n","info.show()\n","print(\"band_instruments:\")\n","band_instruments.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2S0Jk6my2UjQ","executionInfo":{"status":"ok","timestamp":1739509668528,"user_tz":-480,"elapsed":12208,"user":{"displayName":"TAN SWEE NEO KATHLEEN","userId":"08125048316560176091"}},"outputId":"de51e7b4-3054-4cc4-d901-d4462d0bb804"},"id":"2S0Jk6my2UjQ","execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["info:\n","+------+--------------+----------+--------+\n","|  name|          band|      born|children|\n","+------+--------------+----------+--------+\n","|  Mick|Rolling Stones|1943-07-26|    true|\n","|  John|       Beatles|1940-09-10|    true|\n","|  Paul|       Beatles|1942-06-18|    true|\n","|George|       Beatles|1943-02-25|    true|\n","| Ringo|       Beatles|1940-07-07|    true|\n","+------+--------------+----------+--------+\n","\n","band_instruments:\n","+-----+------+\n","| name| plays|\n","+-----+------+\n","| John|guitar|\n","| Paul|  bass|\n","|Keith|guitar|\n","+-----+------+\n","\n"]}]},{"cell_type":"code","source":["# a. Inner join (the default)\n","info.join(band_instruments, on = 'name', how = 'inner')\\\n","    .show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tqC6zuuO2OnB","executionInfo":{"status":"ok","timestamp":1739509676684,"user_tz":-480,"elapsed":3703,"user":{"displayName":"TAN SWEE NEO KATHLEEN","userId":"08125048316560176091"}},"outputId":"4dbb86df-8119-4db8-8a5a-3cd44d91efad"},"id":"tqC6zuuO2OnB","execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["+----+-------+----------+--------+------+\n","|name|   band|      born|children| plays|\n","+----+-------+----------+--------+------+\n","|John|Beatles|1940-09-10|    true|guitar|\n","|Paul|Beatles|1942-06-18|    true|  bass|\n","+----+-------+----------+--------+------+\n","\n"]}]},{"cell_type":"code","source":["# b. Left anti join\n","info.join(band_instruments, on = 'name', how = 'leftanti')\\\n","    .show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N1YmtF__2OuQ","executionInfo":{"status":"ok","timestamp":1739509686091,"user_tz":-480,"elapsed":1968,"user":{"displayName":"TAN SWEE NEO KATHLEEN","userId":"08125048316560176091"}},"outputId":"227150a0-3ed4-4468-fc71-4364555cb851"},"id":"N1YmtF__2OuQ","execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["+------+--------------+----------+--------+\n","|  name|          band|      born|children|\n","+------+--------------+----------+--------+\n","|  Mick|Rolling Stones|1943-07-26|    true|\n","| Ringo|       Beatles|1940-07-07|    true|\n","|George|       Beatles|1943-02-25|    true|\n","+------+--------------+----------+--------+\n","\n"]}]},{"cell_type":"code","source":["# c. Left outer join\n","info.join(band_instruments, on = 'name', how = 'leftouter')\\\n","    .show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AQSNdaSW2Oxa","executionInfo":{"status":"ok","timestamp":1739509694356,"user_tz":-480,"elapsed":1585,"user":{"displayName":"TAN SWEE NEO KATHLEEN","userId":"08125048316560176091"}},"outputId":"ed03e585-8e08-4d83-ca00-ceb14eaab7c5"},"id":"AQSNdaSW2Oxa","execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["+------+--------------+----------+--------+------+\n","|  name|          band|      born|children| plays|\n","+------+--------------+----------+--------+------+\n","|  John|       Beatles|1940-09-10|    true|guitar|\n","|  Mick|Rolling Stones|1943-07-26|    true|  NULL|\n","| Ringo|       Beatles|1940-07-07|    true|  NULL|\n","|George|       Beatles|1943-02-25|    true|  NULL|\n","|  Paul|       Beatles|1942-06-18|    true|  bass|\n","+------+--------------+----------+--------+------+\n","\n"]}]},{"cell_type":"code","source":["# d. Right outer join\n","info.join(band_instruments, on = 'name', how = 'rightouter')\\\n","    .show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nzdydlVn2O00","executionInfo":{"status":"ok","timestamp":1739509704608,"user_tz":-480,"elapsed":1590,"user":{"displayName":"TAN SWEE NEO KATHLEEN","userId":"08125048316560176091"}},"outputId":"5fb228a5-4a48-4ae4-ca4a-76b9a94d2200"},"id":"nzdydlVn2O00","execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["+-----+-------+----------+--------+------+\n","| name|   band|      born|children| plays|\n","+-----+-------+----------+--------+------+\n","| John|Beatles|1940-09-10|    true|guitar|\n","|Keith|   NULL|      NULL|    NULL|guitar|\n","| Paul|Beatles|1942-06-18|    true|  bass|\n","+-----+-------+----------+--------+------+\n","\n"]}]},{"cell_type":"code","source":["# e. Full outer join\n","info.join(band_instruments, on = 'name', how = 'fullouter')\\\n","    .show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vgpgjThi2O3g","executionInfo":{"status":"ok","timestamp":1739509713860,"user_tz":-480,"elapsed":1573,"user":{"displayName":"TAN SWEE NEO KATHLEEN","userId":"08125048316560176091"}},"outputId":"7f5a2cdf-cb5d-488e-9f09-319dba8a7edd"},"id":"vgpgjThi2O3g","execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["+------+--------------+----------+--------+------+\n","|  name|          band|      born|children| plays|\n","+------+--------------+----------+--------+------+\n","|George|       Beatles|1943-02-25|    true|  NULL|\n","|  John|       Beatles|1940-09-10|    true|guitar|\n","| Keith|          NULL|      NULL|    NULL|guitar|\n","|  Mick|Rolling Stones|1943-07-26|    true|  NULL|\n","|  Paul|       Beatles|1942-06-18|    true|  bass|\n","| Ringo|       Beatles|1940-07-07|    true|  NULL|\n","+------+--------------+----------+--------+------+\n","\n"]}]},{"cell_type":"code","source":["# f. Left Semi Join\n","info.join(band_instruments, on = 'name', how = 'leftsemi')\\\n","    .show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nACyR4NO2O6i","executionInfo":{"status":"ok","timestamp":1739509724153,"user_tz":-480,"elapsed":2004,"user":{"displayName":"TAN SWEE NEO KATHLEEN","userId":"08125048316560176091"}},"outputId":"e410c0e4-4d45-4ee9-88af-84aced3d5efc"},"id":"nACyR4NO2O6i","execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["+----+-------+----------+--------+\n","|name|   band|      born|children|\n","+----+-------+----------+--------+\n","|John|Beatles|1940-09-10|    true|\n","|Paul|Beatles|1942-06-18|    true|\n","+----+-------+----------+--------+\n","\n"]}]},{"cell_type":"code","source":["# g. Cross Join\n","info.crossJoin(band_instruments)\\\n","    .show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1417wLE72O9g","executionInfo":{"status":"ok","timestamp":1739509734477,"user_tz":-480,"elapsed":2054,"user":{"displayName":"TAN SWEE NEO KATHLEEN","userId":"08125048316560176091"}},"outputId":"65fa1ed8-c403-4de8-fd7c-b0da55182dd0"},"id":"1417wLE72O9g","execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["+------+--------------+----------+--------+-----+------+\n","|  name|          band|      born|children| name| plays|\n","+------+--------------+----------+--------+-----+------+\n","|  Mick|Rolling Stones|1943-07-26|    true| John|guitar|\n","|  John|       Beatles|1940-09-10|    true| John|guitar|\n","|  Mick|Rolling Stones|1943-07-26|    true| Paul|  bass|\n","|  Mick|Rolling Stones|1943-07-26|    true|Keith|guitar|\n","|  John|       Beatles|1940-09-10|    true| Paul|  bass|\n","|  John|       Beatles|1940-09-10|    true|Keith|guitar|\n","|  Paul|       Beatles|1942-06-18|    true| John|guitar|\n","|George|       Beatles|1943-02-25|    true| John|guitar|\n","| Ringo|       Beatles|1940-07-07|    true| John|guitar|\n","|  Paul|       Beatles|1942-06-18|    true| Paul|  bass|\n","|  Paul|       Beatles|1942-06-18|    true|Keith|guitar|\n","|George|       Beatles|1943-02-25|    true| Paul|  bass|\n","|George|       Beatles|1943-02-25|    true|Keith|guitar|\n","| Ringo|       Beatles|1940-07-07|    true| Paul|  bass|\n","| Ringo|       Beatles|1940-07-07|    true|Keith|guitar|\n","+------+--------------+----------+--------+-----+------+\n","\n"]}]},{"cell_type":"code","execution_count":null,"id":"f43e307c-e880-4559-bbe4-bd55b41e6b83","metadata":{"id":"f43e307c-e880-4559-bbe4-bd55b41e6b83"},"outputs":[],"source":["spark.stop()"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"de-venv","language":"python","name":"de-venv"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":5}