# PRACTICAL 6: MapReduce (MR)

1. Ensure that the hadoop services that are running using user hduser account.

2. Change ownership and permissions for the /tmp directory in HDFS

   i. List the current permissions of the root directory directory
      ~~~bash
      hduser@MyPC:~$ hdfs dfs -ls /
      Found 3 items
      drwxr-xr-x  - hduser supergroup    0 2024-08-15 10:15 /hbase
      drwxrwxr-x  - hduser supergroup    0 2024-08-15 11:04 /tmp
      drwxrwxr-x  - hduser supergroup    0 2024-08-15 08:08 /user
      ~~~
      
     ii. Change the ownership of the /tmp directory to hduser:hduser

     iii. Change the permission of the /tmp directory to drwxrwxr-x 

3. MR Job Example: Filtering Files Containing a Word Pattern using user student account

   i. Prepare data files in HDFS
      ~~~bash
      student@MyPC:~$ hdfs dfs -mkdir input
      student@MyPC:~$ hdfs dfs -put $HADOOP_HOME/etc/hadoop/*xml input
      ~~~
      > Check the list of files in the input directory.
      > 
      > capacity-scheduler.xml
      > 
      > core-site.xml
      > 
      > hadoop-policy.xml
      > 
      > hdfs-rbf-site.xml
      > 
      > hdfs-site.xml
      > 
      > httpfs-site.xml
      > 
      > kms-acls.xml
      > 
      > kms-site.xml
      > 
      > mapred-site.xml
      > 
      > yarn-site.xml 

   ii. Run the MR job to filter files containing the prefix “dfs”:
      ~~~bash
      student@MyPC:~$ hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.6.jar grep input output 'dfs[a-z.]+'
      ~~~
   iii. Check the results:
      ~~~bash
      student@MyPC:~$ hdfs dfs -cat output/*
      1       dfsadmin
      1       dfs.replication
      1       dfs.namenode.name.dir
      1       dfs.datanode.data.dir 
      ~~~

4. MR Job Example: Word Count Example using user student account
(Adapted from https://www.geeksforgeeks.org/hadoop-streaming-using-python-word-count-problem/ )

   i. Copy the wordcount folder to the home directory of student in the local file system.

   ii. Change ownership of the wordcount folder in the local file system to student:hduser.

   iii. Change directory to the wordcount folder in the local file system.

   iv. Change the file permissions of mapper.py and reducer.py in the local file system so that they are executable.
      ~~~bash
      student@MyPC:~$ sudo chmod +x mapper.py
      student@MyPC:~$ sudo chmod +x reducer.py
      ~~~
      
   v. Create a directory named wordcount in HDFS. 

   vi. Copy weather.txt to the HDFS’ wordcount directory

   vii. Execute the Streaming job against the cluster:
      ~~~bash
      student@MyPC:~/wordcount$ hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-*.jar -input wordcount/weather.txt -output wordcount/results -mapper mapper.py -reducer reducer.py -file mapper.py -file reducer.py
      ~~~

   viii. List the contents of the HDFS’ wordcount directory
      ~~~bash
      student@MyPC:~$ hdfs dfs -ls wordcount
      ~~~

   ix. List the contents of the HDFS’ wordcount/results directory
      ~~~bash
      student@MyPC:~$ hdfs dfs -ls wordcount/results
      Found 2 items
      -rw-r--r--  1 student hduser   0  2024-08-20 10:27 wordcount/results/_SUCCESS
      -rw-r--r--  1 student hduser 462  2024-08-20 10:27 wordcount/results/part-00000
      ~~~


   x. View the content of the beginning of the part-00000 file
      ~~~bash
      student@MyPC:~$ hdfs dfs -head wordcount/results/part-00000
      ~~~
