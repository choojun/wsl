# PRACTICAL 3.1: PySpark Shell

~~~ As the user hduser ~~~
ðŸ’¡ Start up HDFS and YARN. 

~~~ As the user student ~~~ 
Add Spark-related variables to studentâ€™s profile 
Open ~/.profile 
     student@PC25:~$ nano ~/.profile
and add the following environment variables to the end of the file:
   export SPARK_HOME=/home/hduser/spark
   export PATH=$PATH:$SPARK_HOME/bin

Source the ~/.profile file.
   student@PC25:~$ source ~/.profile


Using the PySpark Interactive Shell: Word count example

Launch the PySpark interactive shell
$ pyspark





Create an RDD with data from a text file: 
Note: The shakespeare.txt was already downloaded in Practical 2. 
>>> text = sc.textFile("shakespeare.txt")
>>> print(text)



Transform the RDD to implement the word count application using Spark: 
>>> from operator import add
>>> def tokenize(text):
...     return text.split()
...
>>> words = text.flatMap(tokenize)

Apply the map:
>>> wc = words.map(lambda x: (x, 1))
>>> print(wc.toDebugString())



Apply the reduceByKey action to obtain the word counts and save the results in a text file: 
>>> counts = wc.reduceByKey(add)
   >>> counts.saveAsTextFile("wc")
Note:	To explicitly indicate the HDFS path, use "hdfs://localhost:9000/user/student/wc" as the output file name:


Exit the PySpark interactive shell
>>> exit()

Check the HDFS current working directory for the newly created directoryâ€™s contents:
student@PC25:~$ hdfs dfs -ls wc


Use the head command to view one of the part files: 
$ hdfs dfs -head wc/part-00000




ã€°ã€°ã€°ã€°ã€°ã€° End of Practical ã€°ã€°ã€°ã€°ã€°ã€°

ðŸ’¡ If no more activity using Spark or Hadoop, remember to stop the YARN service followed by the HDFS service.


