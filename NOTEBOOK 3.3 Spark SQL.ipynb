{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f9b4e65-714c-46a7-b0ca-2c22d87e349e",
     "showTitle": false,
     "title": ""
    },
    "id": "mEgsCltt_BVP"
   },
   "source": [
    "# NOTEBOOK 3.3 Spark SQL\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/12 15:50:29 WARN Utils: Your hostname, PC25. resolves to a loopback address: 127.0.1.1; using 192.168.76.195 instead (on interface eth0)\n",
      "25/06/12 15:50:29 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/12 15:50:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Read from CSV file with DDL string as schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4b6f89b-2d68-4937-a30a-ac64ef7caa49",
     "showTitle": true,
     "title": "Create Salary dataframe"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 37405,
     "status": "ok",
     "timestamp": 1749541895427,
     "user": {
      "displayName": "TAN SWEE NEO KATHLEEN",
      "userId": "08125048316560176091"
     },
     "user_tz": -480
    },
    "id": "aI4BccXM_BVW",
    "outputId": "fe67db03-d4a1-4dbb-bda7-7a86c4d2ce5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----------+------+---+\n",
      "| ID|Employee|Department|Salary|Age|\n",
      "+---+--------+----------+------+---+\n",
      "|  1|    John| Field-eng|3500.0| 40|\n",
      "|  7|  Martin|   Finance|3500.0| 26|\n",
      "|  3|   Maria|   Finance|3500.0| 28|\n",
      "|  4| Michael|     Sales|3000.0| 20|\n",
      "|  5|   Kelly|   Finance|3500.0| 35|\n",
      "|  2|  Robert|     Sales|4000.0| 38|\n",
      "|  6|    Kate|   Finance|3000.0| 45|\n",
      "|  8|   Kiran|     Sales|2200.0| 35|\n",
      "+---+--------+----------+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = \"ID INT, Employee STRING, Department STRING, Salary DOUBLE, Age INT\"\n",
    "salary_df = spark.read.csv('data/salary_data2.csv', header=True, schema=schema)\n",
    "\n",
    "salary_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f186780-97f0-4b7f-af17-c33073c872ce",
     "showTitle": true,
     "title": "# Perform transformations on the loaded data"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 858,
     "status": "ok",
     "timestamp": 1749541902115,
     "user": {
      "displayName": "TAN SWEE NEO KATHLEEN",
      "userId": "08125048316560176091"
     },
     "user_tz": -480
    },
    "id": "LEVRl1da_BVb",
    "outputId": "8610829f-212a-4ca0-bcb5-e382f96cb2c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----------+------+---+\n",
      "| ID|Employee|Department|Salary|Age|\n",
      "+---+--------+----------+------+---+\n",
      "|  1|    John| Field-eng|3500.0| 40|\n",
      "|  7|  Martin|   Finance|3500.0| 26|\n",
      "|  3|   Maria|   Finance|3500.0| 28|\n",
      "|  5|   Kelly|   Finance|3500.0| 35|\n",
      "|  2|  Robert|     Sales|4000.0| 38|\n",
      "+---+--------+----------+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Perform transformations on the loaded data\n",
    "filtered_df = salary_df.filter(salary_df[\"Salary\"] > 3000)\n",
    "filtered_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Temporary Views\n",
    "Temporary views enables developers us run SQL queries in a program and get the result as a DataFrame. There are 2 types of temporary views:\n",
    "- **Local Temporary View**\n",
    "    - The default temporary view.\n",
    "    - Lasts for the session in which they are created. \n",
    "- **Global Temporary View**\n",
    "    - If we want to have views available across various sessions, we need to create Global Temporary Views. The view definition is stored in the default database, **global_temp**. Once a view is created, we need to use the fully qualified name to access it in a query.\n",
    "\n",
    "### 2.1 Create (Local) Temporary View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----------+------+---+\n",
      "| ID|Employee|Department|Salary|Age|\n",
      "+---+--------+----------+------+---+\n",
      "|  1|    John| Field-eng|3500.0| 40|\n",
      "|  7|  Martin|   Finance|3500.0| 26|\n",
      "|  3|   Maria|   Finance|3500.0| 28|\n",
      "|  5|   Kelly|   Finance|3500.0| 35|\n",
      "|  2|  Robert|     Sales|4000.0| 38|\n",
      "+---+--------+----------+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Save the processed data as a table\n",
    "filtered_df.createOrReplaceTempView(\"high_salary_employees\")\n",
    "\n",
    "# Perform SQL queries on the saved table\n",
    "results_df = spark.sql(\"SELECT * FROM high_salary_employees \")\n",
    "results_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Create Global Temporary View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from current spark session\n",
      "+---+--------+----------+------+---+\n",
      "| ID|Employee|Department|Salary|Age|\n",
      "+---+--------+----------+------+---+\n",
      "|  1|    John| Field-eng|3500.0| 40|\n",
      "|  7|  Martin|   Finance|3500.0| 26|\n",
      "|  3|   Maria|   Finance|3500.0| 28|\n",
      "|  4| Michael|     Sales|3000.0| 20|\n",
      "|  5|   Kelly|   Finance|3500.0| 35|\n",
      "|  2|  Robert|     Sales|4000.0| 38|\n",
      "|  6|    Kate|   Finance|3000.0| 45|\n",
      "|  8|   Kiran|     Sales|2200.0| 35|\n",
      "+---+--------+----------+------+---+\n",
      "\n",
      "Data from a new spark session\n",
      "+---+--------+----------+------+---+\n",
      "| ID|Employee|Department|Salary|Age|\n",
      "+---+--------+----------+------+---+\n",
      "|  1|    John| Field-eng|3500.0| 40|\n",
      "|  7|  Martin|   Finance|3500.0| 26|\n",
      "|  3|   Maria|   Finance|3500.0| 28|\n",
      "|  4| Michael|     Sales|3000.0| 20|\n",
      "|  5|   Kelly|   Finance|3500.0| 35|\n",
      "|  2|  Robert|     Sales|4000.0| 38|\n",
      "|  6|    Kate|   Finance|3000.0| 45|\n",
      "|  8|   Kiran|     Sales|2200.0| 35|\n",
      "+---+--------+----------+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "salary_df.createGlobalTempView(\"salaries\")\n",
    "\n",
    "print(\"Data from current spark session\")\n",
    "spark.sql(\"SELECT * FROM global_temp.salaries\").show()\n",
    "\n",
    "# Global temporary view is tied to a system database `global_temp`\n",
    "print(\"Data from a new spark session\")\n",
    "spark.newSession().sql(\"SELECT * FROM global_temp.salaries\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Running SQL Queries Programmatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "022c96b3-91f2-41ea-91c4-58ecd511a2f6",
     "showTitle": true,
     "title": "Saving Transformed Data as a View"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1269,
     "status": "ok",
     "timestamp": 1749541903387,
     "user": {
      "displayName": "TAN SWEE NEO KATHLEEN",
      "userId": "08125048316560176091"
     },
     "user_tz": -480
    },
    "id": "Y6-VeJr2_BVb",
    "outputId": "22571d4f-f70c-4e2c-d40d-f258de163c90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+------+---+\n",
      "|Employee|Department|Salary|Age|\n",
      "+--------+----------+------+---+\n",
      "|    John| Field-eng|3500.0| 40|\n",
      "|   Kelly|   Finance|3500.0| 35|\n",
      "|  Robert|     Sales|4000.0| 38|\n",
      "|    Kate|   Finance|3000.0| 45|\n",
      "|   Kiran|     Sales|2200.0| 35|\n",
      "+--------+----------+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "salary_df.createOrReplaceTempView(\"employees\")\n",
    "\n",
    "# Select columns\n",
    "sql_string = \"SELECT Employee, Department, Salary, Age FROM employees WHERE age > 30\"\n",
    "results_df = spark.sql(sql_string)\n",
    "results_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "174c84ac-fa9e-42a7-a3b5-a166193e63b0",
     "showTitle": true,
     "title": "Aggregating data"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1638,
     "status": "ok",
     "timestamp": 1749541905028,
     "user": {
      "displayName": "TAN SWEE NEO KATHLEEN",
      "userId": "08125048316560176091"
     },
     "user_tz": -480
    },
    "id": "Y0x62lrD_BVc",
    "outputId": "ce14092c-d142-45f9-e033-d62e18601d20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|average_salary|\n",
      "+--------------+\n",
      "|        3275.0|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Perform an aggregation to calculate the average salary\n",
    "sql_string = \"SELECT AVG(Salary) AS average_salary FROM employees\"\n",
    "average_salary_df = spark.sql(sql_string)\n",
    "average_salary_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97bfd3f0-1a65-4a58-bdc7-58b55dd58840",
     "showTitle": true,
     "title": "Sorting data"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 825,
     "status": "ok",
     "timestamp": 1749541905851,
     "user": {
      "displayName": "TAN SWEE NEO KATHLEEN",
      "userId": "08125048316560176091"
     },
     "user_tz": -480
    },
    "id": "K4GBSz3L_BVd",
    "outputId": "ca047980-b18a-4d3c-fa97-08263e240dc9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----------+------+---+\n",
      "| ID|Employee|Department|Salary|Age|\n",
      "+---+--------+----------+------+---+\n",
      "|  2|  Robert|     Sales|4000.0| 38|\n",
      "|  3|   Maria|   Finance|3500.0| 28|\n",
      "|  5|   Kelly|   Finance|3500.0| 35|\n",
      "|  7|  Martin|   Finance|3500.0| 26|\n",
      "|  1|    John| Field-eng|3500.0| 40|\n",
      "|  6|    Kate|   Finance|3000.0| 45|\n",
      "|  4| Michael|     Sales|3000.0| 20|\n",
      "|  8|   Kiran|     Sales|2200.0| 35|\n",
      "+---+--------+----------+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sort the data based on the salary column in descending order\n",
    "sql_string = \"SELECT * FROM employees ORDER BY Salary DESC\"\n",
    "sorted_df = spark.sql(sql_string)\n",
    "sorted_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38606a23-aa13-49ca-b7dc-d669dd472f55",
     "showTitle": true,
     "title": "Combining Aggregations"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 794,
     "status": "ok",
     "timestamp": 1749541906647,
     "user": {
      "displayName": "TAN SWEE NEO KATHLEEN",
      "userId": "08125048316560176091"
     },
     "user_tz": -480
    },
    "id": "_c4R8vcD_BVd",
    "outputId": "16d9ee9c-b9db-4b2d-e059-36baf3d3092e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+------+---+\n",
      "|Employee|Department|Salary|Age|\n",
      "+--------+----------+------+---+\n",
      "|  Robert|     Sales|4000.0| 38|\n",
      "|    John| Field-eng|3500.0| 40|\n",
      "|   Kelly|   Finance|3500.0| 35|\n",
      "+--------+----------+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sort the data based on the salary column in descending order\n",
    "sql_string = \"\"\"SELECT Employee, Department, Salary, Age FROM employees\n",
    "WHERE age > 30 AND Salary > 3000 ORDER BY Salary DESC\"\"\"\n",
    "\n",
    "filtered_df = spark.sql(sql_string)\n",
    "filtered_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7701c892-0883-4bc2-9b5e-f51cf55fcc78",
     "showTitle": true,
     "title": "Grouping data"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1678,
     "status": "ok",
     "timestamp": 1749541908318,
     "user": {
      "displayName": "TAN SWEE NEO KATHLEEN",
      "userId": "08125048316560176091"
     },
     "user_tz": -480
    },
    "id": "55-7hwig_BVe",
    "outputId": "d8723ee8-049c-4ce0-dedd-3949090ad101"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|Department|       avg(Salary)|\n",
      "+----------+------------------+\n",
      "| Field-eng|            3500.0|\n",
      "|   Finance|            3375.0|\n",
      "|     Sales|3066.6666666666665|\n",
      "+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Group the data based on the Department column and take average salary for each department\n",
    "sql_string = \"SELECT Department, avg(Salary) FROM employees GROUP BY Department\"\n",
    "grouped_df = spark.sql(sql_string)\n",
    "grouped_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abafb986-88fa-4c43-8f65-6f7bbfa70ec6",
     "showTitle": true,
     "title": "Grouping with multiple aggregations"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1111,
     "status": "ok",
     "timestamp": 1749541909431,
     "user": {
      "displayName": "TAN SWEE NEO KATHLEEN",
      "userId": "08125048316560176091"
     },
     "user_tz": -480
    },
    "id": "B8iviOku_BVf",
    "outputId": "c64f988d-3ebe-4145-b288-2ca0b15dd09a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----------+\n",
      "|Department|total_salary|max_salary|\n",
      "+----------+------------+----------+\n",
      "| Field-eng|      3500.0|    3500.0|\n",
      "|   Finance|     13500.0|    3500.0|\n",
      "|     Sales|      9200.0|    4000.0|\n",
      "+----------+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Perform grouping and multiple aggregations\n",
    "sql_string = \"SELECT Department, sum(Salary) AS total_salary, max(Salary) AS max_salary FROM employees GROUP BY Department\"\n",
    "aggregated_df = spark.sql(sql_string)\n",
    "aggregated_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b85baa3-2e70-4038-af6c-440346d96d78",
     "showTitle": true,
     "title": "Window functions"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1635,
     "status": "ok",
     "timestamp": 1749541911068,
     "user": {
      "displayName": "TAN SWEE NEO KATHLEEN",
      "userId": "08125048316560176091"
     },
     "user_tz": -480
    },
    "id": "WxR5_kfD_BVg",
    "outputId": "131b2117-18ac-4c73-b81d-14524717309e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----------+------+---+--------------+\n",
      "| ID|Employee|Department|Salary|Age|cumulative_sum|\n",
      "+---+--------+----------+------+---+--------------+\n",
      "|  1|    John| Field-eng|3500.0| 40|        3500.0|\n",
      "|  7|  Martin|   Finance|3500.0| 26|        3500.0|\n",
      "|  3|   Maria|   Finance|3500.0| 28|        7000.0|\n",
      "|  5|   Kelly|   Finance|3500.0| 35|       10500.0|\n",
      "|  6|    Kate|   Finance|3000.0| 45|       13500.0|\n",
      "|  4| Michael|     Sales|3000.0| 20|        3000.0|\n",
      "|  8|   Kiran|     Sales|2200.0| 35|        5200.0|\n",
      "|  2|  Robert|     Sales|4000.0| 38|        9200.0|\n",
      "+---+--------+----------+------+---+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "# Define the window specification\n",
    "window_spec = Window.partitionBy(\"Department\").orderBy(\"Age\")\n",
    "\n",
    "# Calculate the cumulative sum using window function\n",
    "cumulative_sum_df = salary_df.withColumn(\"cumulative_sum\", sum(col(\"Salary\")).over(window_spec))\n",
    "cumulative_sum_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. More Examples on Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+----------+--------+\n",
      "|code|description|unit_price|quantity|\n",
      "+----+-----------+----------+--------+\n",
      "|1005|        pen|       2.5|       4|\n",
      "|1007|     pencil|       1.0|      10|\n",
      "|1001|   notebook|       5.0|       2|\n",
      "|1003|      ruler|       1.0|       1|\n",
      "|1002| calculator|      55.0|       1|\n",
      "+----+-----------+----------+--------+\n",
      "\n",
      "root\n",
      " |-- code: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- unit_price: string (nullable = true)\n",
      " |-- quantity: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df = spark.read.option(\"sep\", \"\\t\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .csv(\"data/sales.csv\")\n",
    "\n",
    "sales_df.show()\n",
    "sales_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Use SparkSQL to read the columns with correct data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- code: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- unit_price: double (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      "\n",
      "+----+-----------+----------+--------+\n",
      "|code|description|unit_price|quantity|\n",
      "+----+-----------+----------+--------+\n",
      "|1005|        pen|       2.5|       4|\n",
      "|1007|     pencil|       1.0|      10|\n",
      "|1001|   notebook|       5.0|       2|\n",
      "|1003|      ruler|       1.0|       1|\n",
      "|1002| calculator|      55.0|       1|\n",
      "+----+-----------+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df.createOrReplaceTempView('sales')\n",
    "sales_df = spark.sql(\"SELECT code, description, DOUBLE(unit_price), INT(quantity) from sales\")\n",
    "sales_df.printSchema()\n",
    "sales_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---------------+-----------------+--------------------+-------------+-----------------+-------------+----+--------------+---------+-------------+\n",
      "|                 _id|  amazon_product_url|         author| bestsellers_date|         description|        price|   published_date|    publisher|rank|rank_last_week|    title|weeks_on_list|\n",
      "+--------------------+--------------------+---------------+-----------------+--------------------+-------------+-----------------+-------------+----+--------------+---------+-------------+\n",
      "|{5b4aa4ead3089013...|http://www.amazon...|  Dean R Koontz|{{1211587200000}}|Odd Thomas, who c...|   {NULL, 27}|{{1212883200000}}|       Bantam| {1}|           {0}|ODD HOURS|          {1}|\n",
      "|{5b4aa4ead3089013...|http://www.amazon...|Stephenie Meyer|{{1211587200000}}|Aliens have taken...|{25.99, NULL}|{{1212883200000}}|Little, Brown| {2}|           {1}| THE HOST|          {3}|\n",
      "+--------------------+--------------------+---------------+-----------------+--------------------+-------------+-----------------+-------------+----+--------------+---------+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.json(\"data/nyt2.json\")\n",
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---------------+-----------------+--------------------+-------------+-----------------+-------------+----+--------------+--------------------+-------------+\n",
      "|                 _id|  amazon_product_url|         author| bestsellers_date|         description|        price|   published_date|    publisher|rank|rank_last_week|               title|weeks_on_list|\n",
      "+--------------------+--------------------+---------------+-----------------+--------------------+-------------+-----------------+-------------+----+--------------+--------------------+-------------+\n",
      "|{5b4aa4ead3089013...|http://www.amazon...|  Dean R Koontz|{{1211587200000}}|Odd Thomas, who c...|   {NULL, 27}|{{1212883200000}}|       Bantam| {1}|           {0}|           ODD HOURS|          {1}|\n",
      "|{5b4aa4ead3089013...|http://www.amazon...|Stephenie Meyer|{{1211587200000}}|Aliens have taken...|{25.99, NULL}|{{1212883200000}}|Little, Brown| {2}|           {1}|            THE HOST|          {3}|\n",
      "|{5b4aa4ead3089013...|http://www.amazon...|   Emily Giffin|{{1211587200000}}|A woman's happy m...|{24.95, NULL}|{{1212883200000}}| St. Martin's| {3}|           {2}|LOVE THE ONE YOU'...|          {2}|\n",
      "+--------------------+--------------------+---------------+-----------------+--------------------+-------------+-----------------+-------------+----+--------------+--------------------+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Registering the table\n",
    "df.createOrReplaceTempView(\"books\")\n",
    "\n",
    "# Select all the rows from book_df and display the first 3 rows\n",
    "spark.sql(\"select * from books\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|         Themes|count|\n",
      "+---------------+-----+\n",
      "|    Anger_Theme|  203|\n",
      "|   Other_Themes| 8778|\n",
      "|  Mystery_Theme|  454|\n",
      "|     Hate_Theme|   23|\n",
      "|     Love_Theme|  392|\n",
      "|Happiness_Theme|   34|\n",
      "|   Horror_Theme|    6|\n",
      "| Criminal_Theme|  305|\n",
      "+---------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Summarize the number of books by themes\n",
    "spark.sql(\"select \\\n",
    "CASE WHEN description LIKE '%love%' THEN 'Love_Theme' \\\n",
    "WHEN description LIKE '%hate%' THEN 'Hate_Theme' \\\n",
    "WHEN description LIKE '%happy%' THEN 'Happiness_Theme' \\\n",
    "WHEN description LIKE '%anger%' THEN 'Anger_Theme' \\\n",
    "WHEN description LIKE '%horror%' THEN 'Horror_Theme' \\\n",
    "WHEN description LIKE '%death%' THEN 'Criminal_Theme' \\\n",
    "WHEN description LIKE '%detective%' THEN 'Mystery_Theme' \\\n",
    "ELSE 'Other_Themes' \\\n",
    "END Themes \\\n",
    "from books\").groupBy('Themes').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Caching, Checkpointing, Repartitioning and Coalesing\n",
    "https://spark.apache.org/docs/1.1.1/api/python/pyspark.rdd.RDD-class.html\n",
    "\n",
    "### 5.1 Caching\n",
    "- Running the cache() transformation on a Spark DataFrame stores the DataFrame in memory across the cluster for faster access during ubsequent actions.\n",
    "- In-Memory Storage: Spark stores the partitions of the DataFrame in memory. If there isn’t enough memory, some partitions may be recomputed as needed.\n",
    "    - Performance Gain: Subsequent actions on the cached DataFrame are faster because Spark reuses the in-memory data instead of recomputing it.\n",
    "    - Cluster-Wide Effect: Caching applies across the cluster, meaning each node tries to keep its share of data in memory.\n",
    "\n",
    "#### 5.1(a) cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_id: struct<$oid:string>, amazon_product_url: string, author: string, bestsellers_date: struct<$date:struct<$numberLong:string>>, description: string, price: struct<$numberDouble:string,$numberInt:string>, published_date: struct<$date:struct<$numberLong:string>>, publisher: string, rank: struct<$numberInt:string>, rank_last_week: struct<$numberInt:string>, title: string, weeks_on_list: struct<$numberInt:string>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cache a DataFrame (defaults to MEMORY_AND_DISK)\n",
    "df.cache()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1(b) persist()\n",
    "Used to specify the storage level.\n",
    "\n",
    "**Storage Levels**:\n",
    "- StorageLevel.MEMORY_ONLY\n",
    "- StorageLevel.MEMORY_AND_DISK\n",
    "- StorageLevel.DISK_ONLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/12 15:50:39 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[_id: struct<$oid:string>, amazon_product_url: string, author: string, bestsellers_date: struct<$date:struct<$numberLong:string>>, description: string, price: struct<$numberDouble:string,$numberInt:string>, published_date: struct<$date:struct<$numberLong:string>>, publisher: string, rank: struct<$numberInt:string>, rank_last_week: struct<$numberInt:string>, title: string, weeks_on_list: struct<$numberInt:string>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import StorageLevel\n",
    "\n",
    "# Cache a DataFrame; df.cache() (i.e., without parameters) defaults to MEMORY_AND_DISK\n",
    "df.persist(StorageLevel.MEMORY_ONLY)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1(c) unpersist()\n",
    "\n",
    "Removes a DataFrame or RDD from memory and/or disk where it was cached or persisted. When you no longer need a cached/persisted dataset, calling unpersist() helps free up resources (memory and/or disk), making room for other computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_id: struct<$oid:string>, amazon_product_url: string, author: string, bestsellers_date: struct<$date:struct<$numberLong:string>>, description: string, price: struct<$numberDouble:string,$numberInt:string>, published_date: struct<$date:struct<$numberLong:string>>, publisher: string, rank: struct<$numberInt:string>, rank_last_week: struct<$numberInt:string>, title: string, weeks_on_list: struct<$numberInt:string>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 checkpoint()\n",
    "Steps:\n",
    "1. Set a checkpointing directory using setCheckpointDir() method.\n",
    "2. Store the content of baseRDD using checkpoint()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---------------+-----------------+--------------------+-------------+-----------------+-------------+----+--------------+--------------------+-------------+\n",
      "|                 _id|  amazon_product_url|         author| bestsellers_date|         description|        price|   published_date|    publisher|rank|rank_last_week|               title|weeks_on_list|\n",
      "+--------------------+--------------------+---------------+-----------------+--------------------+-------------+-----------------+-------------+----+--------------+--------------------+-------------+\n",
      "|{5b4aa4ead3089013...|http://www.amazon...|  Dean R Koontz|{{1211587200000}}|Odd Thomas, who c...|   {NULL, 27}|{{1212883200000}}|       Bantam| {1}|           {0}|           ODD HOURS|          {1}|\n",
      "|{5b4aa4ead3089013...|http://www.amazon...|Stephenie Meyer|{{1211587200000}}|Aliens have taken...|{25.99, NULL}|{{1212883200000}}|Little, Brown| {2}|           {1}|            THE HOST|          {3}|\n",
      "|{5b4aa4ead3089013...|http://www.amazon...|   Emily Giffin|{{1211587200000}}|A woman's happy m...|{24.95, NULL}|{{1212883200000}}| St. Martin's| {3}|           {2}|LOVE THE ONE YOU'...|          {2}|\n",
      "+--------------------+--------------------+---------------+-----------------+--------------------+-------------+-----------------+-------------+----+--------------+--------------------+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sc = spark.sparkContext\n",
    "sc.setCheckpointDir(\"checkpoints/checkpointing\")\n",
    "\n",
    "# checkpointing returns a new DataFrame\n",
    "df_cp = df.checkpoint()\n",
    "\n",
    "# trigger it\n",
    "df_cp.count()\n",
    "\n",
    "# use the checkpointed DataFrame\n",
    "df_cp.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 repartition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Increase the number of partitions to 10\n",
    "df.repartition(10).rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 coalesce(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reduce the number of partitions to 1 partition\n",
    "df.coalesce(4).rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Convert Spark DataFrame to RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(books_rdd):  <class 'pyspark.rdd.RDD'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(_id=Row($oid='5b4aa4ead3089013507db18b'), amazon_product_url='http://www.amazon.com/Odd-Hours-Dean-Koontz/dp/0553807056?tag=NYTBS-20', author='Dean R Koontz', bestsellers_date=Row($date=Row($numberLong='1211587200000')), description='Odd Thomas, who can communicate with the dead, confronts evil forces in a California coastal town.', price=Row($numberDouble=None, $numberInt='27'), published_date=Row($date=Row($numberLong='1212883200000')), publisher='Bantam', rank=Row($numberInt='1'), rank_last_week=Row($numberInt='0'), title='ODD HOURS', weeks_on_list=Row($numberInt='1')),\n",
       " Row(_id=Row($oid='5b4aa4ead3089013507db18c'), amazon_product_url='http://www.amazon.com/The-Host-Novel-Stephenie-Meyer/dp/0316218502?tag=NYTBS-20', author='Stephenie Meyer', bestsellers_date=Row($date=Row($numberLong='1211587200000')), description='Aliens have taken control of the minds and bodies of most humans, but one woman won’t surrender.', price=Row($numberDouble='25.99', $numberInt=None), published_date=Row($date=Row($numberLong='1212883200000')), publisher='Little, Brown', rank=Row($numberInt='2'), rank_last_week=Row($numberInt='1'), title='THE HOST', weeks_on_list=Row($numberInt='3'))]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the dataframe into an RDD\n",
    "books_rdd = df.rdd\n",
    "\n",
    "print(\"type(books_rdd): \", type(books_rdd))\n",
    "books_rdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Physical and logical dataframe plans\n",
      "== Physical Plan ==\n",
      "*(1) Project [employee#1, department#2]\n",
      "+- *(1) Filter (isnotnull(Salary#3) AND (Salary#3 > 3500.0))\n",
      "   +- FileScan csv [Employee#1,Department#2,Salary#3] Batched: false, DataFilters: [isnotnull(Salary#3), (Salary#3 > 3500.0)], Format: CSV, Location: InMemoryFileIndex(1 paths)[hdfs://localhost:9000/user/student/data/salary_data2.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Salary), GreaterThan(Salary,3500.0)], ReadSchema: struct<Employee:string,Department:string,Salary:double>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query with Catalyst Optimizer for CSV data\n",
    "result_df = salary_df.select(\"employee\", \"department\").filter(salary_df[\"Salary\"] > 3500)\n",
    "\n",
    "# Explain the optimized query plan\n",
    "print(\"\\nPhysical and logical dataframe plans\")\n",
    "result_df.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/12 16:00:46 WARN MemoryManager: Total allocation exceeds 50.00% (508,559,360 bytes) of heap memory\n",
      "Scaling row group sizes to 94.73% for 4 writers\n",
      "25/06/12 16:00:46 WARN MemoryManager: Total allocation exceeds 50.00% (508,559,360 bytes) of heap memory\n",
      "Scaling row group sizes to 75.78% for 5 writers\n",
      "25/06/12 16:00:46 WARN MemoryManager: Total allocation exceeds 50.00% (508,559,360 bytes) of heap memory\n",
      "Scaling row group sizes to 63.15% for 6 writers\n",
      "25/06/12 16:00:46 WARN MemoryManager: Total allocation exceeds 50.00% (508,559,360 bytes) of heap memory\n",
      "Scaling row group sizes to 54.13% for 7 writers\n",
      "25/06/12 16:00:46 WARN MemoryManager: Total allocation exceeds 50.00% (508,559,360 bytes) of heap memory\n",
      "Scaling row group sizes to 47.36% for 8 writers\n",
      "25/06/12 16:00:46 WARN MemoryManager: Total allocation exceeds 50.00% (508,559,360 bytes) of heap memory\n",
      "Scaling row group sizes to 54.13% for 7 writers\n",
      "25/06/12 16:00:46 WARN MemoryManager: Total allocation exceeds 50.00% (508,559,360 bytes) of heap memory\n",
      "Scaling row group sizes to 63.15% for 6 writers\n",
      "25/06/12 16:00:46 WARN MemoryManager: Total allocation exceeds 50.00% (508,559,360 bytes) of heap memory\n",
      "Scaling row group sizes to 75.78% for 5 writers\n",
      "25/06/12 16:00:46 WARN MemoryManager: Total allocation exceeds 50.00% (508,559,360 bytes) of heap memory\n",
      "Scaling row group sizes to 94.73% for 4 writers\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----------+------+---+\n",
      "| ID|Employee|Department|Salary|Age|\n",
      "+---+--------+----------+------+---+\n",
      "|  1|    John| Field-eng|3500.0| 40|\n",
      "|  7|  Martin|   Finance|3500.0| 26|\n",
      "|  4| Michael|     Sales|3000.0| 20|\n",
      "|  5|   Kelly|   Finance|3500.0| 35|\n",
      "|  3|   Maria|   Finance|3500.0| 28|\n",
      "|  2|  Robert|     Sales|4000.0| 38|\n",
      "|  6|    Kate|   Finance|3000.0| 45|\n",
      "|  8|   Kiran|     Sales|2200.0| 35|\n",
      "+---+--------+----------+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "salary_df.write.parquet(\"data/salary.parquet\")\n",
    "\n",
    "parquet_df = spark.read.parquet('data/salary.parquet')\n",
    "parquet_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Physical and logical dataframe plans\n",
      "== Physical Plan ==\n",
      "*(1) Project [employee#807, department#808]\n",
      "+- *(1) Filter (isnotnull(Salary#809) AND (Salary#809 > 3500.0))\n",
      "   +- *(1) ColumnarToRow\n",
      "      +- FileScan parquet [Employee#807,Department#808,Salary#809] Batched: true, DataFilters: [isnotnull(Salary#809), (Salary#809 > 3500.0)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://localhost:9000/user/student/data/salary.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(Salary), GreaterThan(Salary,3500.0)], ReadSchema: struct<Employee:string,Department:string,Salary:double>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query with Catalyst Optimizer for CSV data\n",
    "parquet_result_df = parquet_df.select(\"employee\", \"department\").filter(parquet_df[\"Salary\"] > 3500)\n",
    "\n",
    "# Explain the optimized query plan\n",
    "print(\"\\nPhysical and logical dataframe plans\")\n",
    "parquet_result_df.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 921,
     "status": "ok",
     "timestamp": 1749541911993,
     "user": {
      "displayName": "TAN SWEE NEO KATHLEEN",
      "userId": "08125048316560176091"
     },
     "user_tz": -480
    },
    "id": "ZQFM07dLIgPG"
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 969987236417588,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "Chapter 6 Code",
   "widgets": {}
  },
  "colab": {
   "provenance": [
    {
     "file_id": "1IXrf7HSAPGxhdsK1E59L61XdtYXlqR_B",
     "timestamp": 1730636753168
    }
   ]
  },
  "kernelspec": {
   "display_name": "de-venv",
   "language": "python",
   "name": "de-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
