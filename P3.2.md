# PRACTICAL 3.2: PySpark Resilient Distributed Datasets (RDDs)

0. Login as the user hduser, and start up HDFS and YARN. 

1. Login as the user student, and put data files in HDFS

   1.1 Download/copy the de folder from the course’s Google Classroom to your C:\ drive.

   1.2 Copy the de folder from Windows to WSL:
      ~~~bash
      student@MyPC:~$ cp -r /mnt/c/Users/choojun/Documents/de /home/student
      ~~~
      > In case you downloaded the de folder with missing data sub-folder initially, and then subsequently downloaded the data folder into the de folder:
      > ~~~bash
      > $ cp -r /mnt/c/Users/choojun/Documents/de/data /home/student/de/
      > ~~~

    1.3 Put the de/data folder in HDFS (this may take some time):
      ~~~bash
      student@PC25:~$ hdfs dfs -put /home/student/de/data .
      ~~~

2. Access JupyterLab

   2.1 Start the JupyterLab service 

   2.2 Access JupyterLab in your browser

3. PySpark RDD

   3.1 Open the Jupyter notebook NOTEBOOK 3.1 Spark RDDs.ipynb in your JupyterLab’s de directory.

   3.2 For each cell, i) review and understand the code, and ii) run the code and observe its output.

# End of Practical

If no more activity using Spark or Hadoop, remember to stop the YARN service followed by the HDFS service.


