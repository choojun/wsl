# PRACTICAL 3.2: PySpark Resilient Distributed Datasets (RDDs)

0. Login as the user hduser, and start up HDFS and YARN. 

1. Login as the user student, and put data files in HDFS

   1.1 Download/copy the de folder from the courseâ€™s Google Classroom to your C:\ drive.

   1.2 Copy the de folder from Windows to WSL:
      ~~~bash
      student@MyPC:~$ sudo cp -r /mnt/c/de /home/student
      ~~~
      > In case you downloaded the de folder with missing data sub-folder initially, and then subsequently downloaded the data folder into the de folder:
      > ~~~bash
      > $ sudo cp -r /mnt/c/de/data /home/student/de/
      > ~~~

    1.3 Put the de/data folder in HDFS (this may take some time):
      ~~~bash
      student@PC25:~$ hdfs dfs -put /home/student/de/data .
      ~~~

Access JupyterLab
Start the JupyterLab service 
Access JupyterLab in your browser


PySpark RDD
Open the Jupyter notebook NOTEBOOK 3.1 Spark RDDs.ipynb in your JupyterLabâ€™s de folder.
For each cell,
Review and understand the code, and
Run the code and observe its output.

ðŸ’¡ If no more activity using Spark or Hadoop, remember to stop the YARN service followed by the HDFS service.


