{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ab57100-6e42-4db3-b774-6eff92a549ff",
   "metadata": {
    "id": "2ab57100-6e42-4db3-b774-6eff92a549ff"
   },
   "source": [
    "# NOTEBOOK 6.1 Spark Structured Streaming\n",
    "Adapted from: Spark - The Definitive Guide by Bill Chambers and Matei Zaharia (2018)\n",
    "\n",
    "This demo requires the folder **activity-data** to be put in the folder named **data** in HDFS. This folder contains the Heterogeneity Human Activity Recognition Dataset which consists of smartphone and smartwatch sensor readings from a variety of devices - specifically, the accelerometer and gyroscope, sampled at the highest possible frequency supported by the devices. Readings from these sensors were recorded while users performed activities like biking, sitting, standing, walking, etc. There were several different smartphones and smartwatches used, anda total of 9 users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a153ed7d-9894-42bc-a3dd-0897328072ef",
   "metadata": {
    "id": "a153ed7d-9894-42bc-a3dd-0897328072ef",
    "outputId": "42c29880-9e99-4fd0-e844-7708c5b352f0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/12 16:22:24 WARN Utils: Your hostname, PC25. resolves to a loopback address: 127.0.1.1; using 192.168.76.195 instead (on interface eth0)\n",
      "25/06/12 16:22:24 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/12 16:22:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"StructuredStreamingDemo\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26b7ade-782c-4dd6-a64e-f11489691ec0",
   "metadata": {
    "id": "f26b7ade-782c-4dd6-a64e-f11489691ec0"
   },
   "source": [
    "## 1. Simulating Streaming\n",
    "\n",
    "### 1.1 Read the static version of the dataset as a DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1a99b43-b782-4bca-bc1a-0f4af97877e8",
   "metadata": {
    "id": "b1a99b43-b782-4bca-bc1a-0f4af97877e8",
    "outputId": "fc548a2d-2c61-4934-e4e2-7475f3cd425d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StructType([StructField('Arrival_Time', LongType(), True), StructField('Creation_Time', LongType(), True), StructField('Device', StringType(), True), StructField('Index', LongType(), True), StructField('Model', StringType(), True), StructField('User', StringType(), True), StructField('gt', StringType(), True), StructField('x', DoubleType(), True), StructField('y', DoubleType(), True), StructField('z', DoubleType(), True)])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activityStaticDf = spark.read.json(\"data/activity-data\")\n",
    "dataSchema = activityStaticDf.schema\n",
    "dataSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a994be1a-494e-4b48-88a4-4fb38b170820",
   "metadata": {
    "id": "a994be1a-494e-4b48-88a4-4fb38b170820",
    "outputId": "a976308b-3637-4fa6-e183-d137eb3c6c88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Arrival_Time: long (nullable = true)\n",
      " |-- Creation_Time: long (nullable = true)\n",
      " |-- Device: string (nullable = true)\n",
      " |-- Index: long (nullable = true)\n",
      " |-- Model: string (nullable = true)\n",
      " |-- User: string (nullable = true)\n",
      " |-- gt: string (nullable = true)\n",
      " |-- x: double (nullable = true)\n",
      " |-- y: double (nullable = true)\n",
      " |-- z: double (nullable = true)\n",
      "\n",
      "+-------------+-------------------+--------+-----+------+----+-----+------------+------------+------------+\n",
      "| Arrival_Time|      Creation_Time|  Device|Index| Model|User|   gt|           x|           y|           z|\n",
      "+-------------+-------------------+--------+-----+------+----+-----+------------+------------+------------+\n",
      "|1424686735090|1424686733090638193|nexus4_1|   18|nexus4|   g|stand| 3.356934E-4|-5.645752E-4|-0.018814087|\n",
      "|1424686735292|1424688581345918092|nexus4_2|   66|nexus4|   g|stand|-0.005722046| 0.029083252| 0.005569458|\n",
      "|1424686735500|1424686733498505625|nexus4_1|   99|nexus4|   g|stand|   0.0078125|-0.017654419| 0.010025024|\n",
      "|1424686735691|1424688581745026978|nexus4_2|  145|nexus4|   g|stand|-3.814697E-4|   0.0184021|-0.013656616|\n",
      "|1424686735890|1424688581945252808|nexus4_2|  185|nexus4|   g|stand|-3.814697E-4|-0.031799316| -0.00831604|\n",
      "+-------------+-------------------+--------+-----+------+----+-----+------------+------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "activityStaticDf.printSchema()\n",
    "activityStaticDf.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9c40e2-a8f4-4096-8602-9e6c4499c014",
   "metadata": {
    "id": "be9c40e2-a8f4-4096-8602-9e6c4499c014"
   },
   "source": [
    "### 1.2 What are the information stored in the DataFrame?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7df935-ed80-4ebd-8d3a-df3b5421b773",
   "metadata": {
    "id": "ac7df935-ed80-4ebd-8d3a-df3b5421b773"
   },
   "source": [
    "### 1.3 Create a streamining version of the same dataset\n",
    "Each input file in the dataset one by one as if it was a stream.\n",
    "\n",
    "**Streaming DataFrames** are similar to static DataFrames. We create them within Spark applications and then perform transformations on them to get the data into the correct format. One small difference between streaming DataFrames and static DataFrames is that Structured Streaming does not allow schema inference without explicitly enabling it. We enable schema inference by setting the configuraiton **spark.sql.streaming.schemaInference** to **true**.\n",
    "\n",
    "Thus, we will read the schema from one file (with a valid schema) and pass the **dataSchema** object from our static DataFrame to our streamining DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a575eca8-91d4-4641-afea-eb605667aa0e",
   "metadata": {
    "id": "a575eca8-91d4-4641-afea-eb605667aa0e"
   },
   "outputs": [],
   "source": [
    "streaming = spark.readStream.schema(dataSchema).option(\"maxFilesPerTrigger\", 1)\\\n",
    "  .json(\"data/activity-data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf79c30-6bb8-4c25-a796-6ee6886fb958",
   "metadata": {
    "id": "aaf79c30-6bb8-4c25-a796-6ee6886fb958"
   },
   "source": [
    "Note: **maxFilesPerTrigger** allows us to control how quickly Spark will read all the files in the folder. In this demo, we set a lower value to limit the flow of the stream to one file per trigger. This is just to demonstrate how Structured Streaming runs incrementally.\n",
    "\n",
    "### 1.4 Group and count data by the **gt** column\n",
    "The **gt** column is the activity being performed by the user at that point in time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05953838-ab96-4137-8827-9d084a30acd2",
   "metadata": {
    "id": "05953838-ab96-4137-8827-9d084a30acd2"
   },
   "outputs": [],
   "source": [
    "activityCounts = streaming.groupBy(\"gt\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53277d25-60e7-4d13-b979-93aaf4e84b30",
   "metadata": {
    "id": "53277d25-60e7-4d13-b979-93aaf4e84b30"
   },
   "source": [
    "#### To avoid too many shuffle partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50d5e957-a3f6-4aa5-9a3b-8c107a4d6565",
   "metadata": {
    "id": "50d5e957-a3f6-4aa5-9a3b-8c107a4d6565"
   },
   "outputs": [],
   "source": [
    "# Set the shuffle partitions to a small value\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6113d9-28c9-448a-b50f-3a7a0b9421a0",
   "metadata": {
    "id": "4d6113d9-28c9-448a-b50f-3a7a0b9421a0"
   },
   "source": [
    "### 1.5 Specify action to start the query.\n",
    "- Output destination for the result of this query: write to a _memory sink_ which keeps an in-memory table of the results.\n",
    "- How Spark will output the data: _complete_ output mode. (This mode rewrites all the keys along with theier counts after every trigger)\n",
    "\n",
    "Once the following code is executed, the streamining computation will be started in the background. The query object **activityQuery** is a handle to that active streamining query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7ec74fb-2a36-4c95-930a-f4091bcbfc8a",
   "metadata": {
    "id": "a7ec74fb-2a36-4c95-930a-f4091bcbfc8a",
    "outputId": "1c5182fa-0033-4c23-ea2e-b61af99c7120"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/12 16:22:33 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-ef32c3e4-a01d-4f31-b3e2-798e04d338e4. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/06/12 16:22:33 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "activityQuery = activityCounts.writeStream.queryName(\"activity_counts\")\\\n",
    "  .format(\"memory\").outputMode(\"complete\")\\\n",
    "  .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3703c53b-74a6-48f9-ac2d-6618885e477a",
   "metadata": {
    "id": "3703c53b-74a6-48f9-ac2d-6618885e477a"
   },
   "source": [
    "### 1.6 Query the in-memory table of the current output of the streaming aggregation\n",
    "Note: the in-memory table has the same name as the stream, i.e. **activity_counts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a64a3dfa-e1a1-4b3c-859f-3c793d0649fc",
   "metadata": {
    "id": "a64a3dfa-e1a1-4b3c-859f-3c793d0649fc",
    "outputId": "fd10c6e2-dad6-426c-e512-a6505effd427"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| gt|count|\n",
      "+---+-----+\n",
      "+---+-----+\n",
      "\n",
      "+----------+------+\n",
      "|        gt| count|\n",
      "+----------+------+\n",
      "|       sit| 98475|\n",
      "|     stand| 91084|\n",
      "|stairsdown| 74913|\n",
      "|      walk|106039|\n",
      "|  stairsup| 83654|\n",
      "|      null| 83566|\n",
      "|      bike| 86368|\n",
      "+----------+------+\n",
      "\n",
      "+----------+------+\n",
      "|        gt| count|\n",
      "+----------+------+\n",
      "|       sit|258492|\n",
      "|     stand|239087|\n",
      "|stairsdown|196627|\n",
      "|      walk|278353|\n",
      "|  stairsup|219602|\n",
      "|      null|219375|\n",
      "|      bike|226720|\n",
      "+----------+------+\n",
      "\n",
      "+----------+------+\n",
      "|        gt| count|\n",
      "+----------+------+\n",
      "|       sit|443119|\n",
      "|     stand|409851|\n",
      "|stairsdown|337074|\n",
      "|      walk|477183|\n",
      "|  stairsup|376463|\n",
      "|      null|376079|\n",
      "|      bike|388672|\n",
      "+----------+------+\n",
      "\n",
      "+----------+------+\n",
      "|        gt| count|\n",
      "+----------+------+\n",
      "|       sit|627756|\n",
      "|     stand|580625|\n",
      "|stairsdown|477537|\n",
      "|      walk|676007|\n",
      "|  stairsup|533326|\n",
      "|      null|532772|\n",
      "|      bike|550608|\n",
      "+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "for i in range(5):\n",
    "    spark.sql(\"SELECT * FROM activity_counts\").show()\n",
    "    sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "732663bf-5dad-41f0-9e40-6860f8e4819b",
   "metadata": {
    "id": "732663bf-5dad-41f0-9e40-6860f8e4819b",
    "outputId": "41e6bc2e-852d-4c9c-b9c7-2d10e04bbed5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<pyspark.sql.streaming.query.StreamingQuery at 0x7f4ef93cc640>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.streams.active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3dde3212-b7ef-439c-9c22-5b7f2b85f816",
   "metadata": {
    "id": "3dde3212-b7ef-439c-9c22-5b7f2b85f816"
   },
   "outputs": [],
   "source": [
    "# # Specify to wait for termination of the query to prevent the driver process from exiting while the query is active.\n",
    "# activityQuery.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41cef41b-a8a2-47cf-be52-9867e61b40ac",
   "metadata": {
    "id": "41cef41b-a8a2-47cf-be52-9867e61b40ac"
   },
   "outputs": [],
   "source": [
    "activityQuery.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674da7a2-d098-4291-902c-c1bd8219e372",
   "metadata": {
    "id": "674da7a2-d098-4291-902c-c1bd8219e372"
   },
   "source": [
    "## 2. Transformations on Streams\n",
    "\n",
    "### 2.1 Selections & Filtering\n",
    "\n",
    "In this demo, we are not updating any keys over time. Therefore, we will use the _Append_ output mode so that new results are appended to the output table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ffb4c62-83c6-4282-89b8-98ea92ecf761",
   "metadata": {
    "id": "5ffb4c62-83c6-4282-89b8-98ea92ecf761",
    "outputId": "d6a4ccbc-2f7a-4d42-a8d6-db5fff266429"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/12 16:22:48 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-c1c7a04a-c028-46cb-b945-9b329418fc98. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/06/12 16:22:48 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "simpleTransform = streaming.withColumn(\"stairs\", expr(\"gt like '%stairs%'\"))\\\n",
    "  .where(\"stairs\")\\\n",
    "  .where(\"gt is not null\")\\\n",
    "  .select(\"gt\", \"model\", \"arrival_time\", \"creation_time\")\\\n",
    "  .writeStream\\\n",
    "  .queryName(\"simple_transform\")\\\n",
    "  .format(\"memory\")\\\n",
    "  .outputMode(\"append\")\\\n",
    "  .start()\n",
    "\n",
    "# simpleTransform.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95c17169-8adf-452e-802e-e877d97d8136",
   "metadata": {
    "id": "95c17169-8adf-452e-802e-e877d97d8136",
    "outputId": "194d7ee5-3ad1-4e6a-d922-938444a1692c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------------+-------------+\n",
      "| gt|model|arrival_time|creation_time|\n",
      "+---+-----+------------+-------------+\n",
      "+---+-----+------------+-------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/12 16:22:52 WARN TaskSetManager: Stage 168 contains a task of very large size (1400 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-------------+-------------------+\n",
      "|      gt| model| arrival_time|      creation_time|\n",
      "+--------+------+-------------+-------------------+\n",
      "|stairsup|nexus4|1424687983758|1424689829813792398|\n",
      "|stairsup|nexus4|1424687984076|1424687982079402816|\n",
      "|stairsup|nexus4|1424687984475|1424687982484364704|\n",
      "|stairsup|nexus4|1424687984878|1424687982887562947|\n",
      "|stairsup|nexus4|1424687985286|1424687983295717413|\n",
      "|stairsup|nexus4|1424687985682|1424687983685793097|\n",
      "|stairsup|nexus4|1424687986090|1424687984093630011|\n",
      "|stairsup|nexus4|1424687986488|1424687984497347052|\n",
      "|stairsup|nexus4|1424687986893|1424687984899471108|\n",
      "|stairsup|nexus4|1424687987295|1424687985302272622|\n",
      "|stairsup|nexus4|1424687987695|1424687985705056175|\n",
      "|stairsup|nexus4|1424687988024|1424689834071137344|\n",
      "|stairsup|nexus4|1424687988225|1424687986233925804|\n",
      "|stairsup|nexus4|1424687988428|1424689834473938858|\n",
      "|stairsup|nexus4|1424687988628|1424689834675293838|\n",
      "|stairsup|nexus4|1424687988830|1424689834876892959|\n",
      "|stairsup|nexus4|1424687989033|1424687987039589866|\n",
      "|stairsup|nexus4|1424687989232|1424687987240969503|\n",
      "|stairsup|nexus4|1424687989434|1424687987442568624|\n",
      "|stairsup|nexus4|1424687989636|1424689835689546051|\n",
      "+--------+------+-------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/12 16:22:56 WARN TaskSetManager: Stage 192 contains a task of very large size (3352 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-------------+-------------------+\n",
      "|      gt| model| arrival_time|      creation_time|\n",
      "+--------+------+-------------+-------------------+\n",
      "|stairsup|nexus4|1424687983758|1424689829813792398|\n",
      "|stairsup|nexus4|1424687984076|1424687982079402816|\n",
      "|stairsup|nexus4|1424687984475|1424687982484364704|\n",
      "|stairsup|nexus4|1424687984878|1424687982887562947|\n",
      "|stairsup|nexus4|1424687985286|1424687983295717413|\n",
      "|stairsup|nexus4|1424687985682|1424687983685793097|\n",
      "|stairsup|nexus4|1424687986090|1424687984093630011|\n",
      "|stairsup|nexus4|1424687986488|1424687984497347052|\n",
      "|stairsup|nexus4|1424687986893|1424687984899471108|\n",
      "|stairsup|nexus4|1424687987295|1424687985302272622|\n",
      "|stairsup|nexus4|1424687987695|1424687985705056175|\n",
      "|stairsup|nexus4|1424687988024|1424689834071137344|\n",
      "|stairsup|nexus4|1424687988225|1424687986233925804|\n",
      "|stairsup|nexus4|1424687988428|1424689834473938858|\n",
      "|stairsup|nexus4|1424687988628|1424689834675293838|\n",
      "|stairsup|nexus4|1424687988830|1424689834876892959|\n",
      "|stairsup|nexus4|1424687989033|1424687987039589866|\n",
      "|stairsup|nexus4|1424687989232|1424687987240969503|\n",
      "|stairsup|nexus4|1424687989434|1424687987442568624|\n",
      "|stairsup|nexus4|1424687989636|1424689835689546051|\n",
      "+--------+------+-------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Note: this may take a while\n",
    "for i in range(3):\n",
    "    spark.sql(\"SELECT * FROM simple_transform\").show()\n",
    "    sleep(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882fd9f2-525b-40f4-94f4-20784024417f",
   "metadata": {
    "id": "882fd9f2-525b-40f4-94f4-20784024417f"
   },
   "source": [
    "3/01/26 11:12:05 WARN TaskSetManager: Stage 272 contains a task of very large size (19820 KiB). The maximum recommended task size is 1000 KiB.                                 (0 + 0) / 1]\n",
    "+--------+------+-------------+-------------------+                                                                                                                                         \n",
    "|      gt| model| arrival_time|      creation_time|\n",
    "+--------+------+-------------+-------------------+\n",
    "|stairsup|nexus4|1424687983719|1424687981726802718|\n",
    "|stairsup|nexus4|1424687984000|1424687982009853255|\n",
    "|stairsup|nexus4|1424687984404|1424687982411977009|\n",
    "|stairsup|nexus4|1424687984805|1424687982814351277|\n",
    "|stairsup|nexus4|1424687985210|1424687983217500861|\n",
    "|stairsup|nexus4|1424687985620|1424687983620332892|\n",
    "|stairsup|nexus4|1424687986016|1424687984023164923|\n",
    "|stairsup|nexus4|1424687986420|1424687984425874884|\n",
    "|stairsup|nexus4|1424687986820|1424687984828822915|\n",
    "|stairsup|nexus4|1424687987225|1424687985231654946|\n",
    "|stairsup|nexus4|1424687987625|1424687985634469017|\n",
    "|stairsup|nexus4|1424687987992|1424687986002114280|\n",
    "|stairsup|nexus4|1424687988191|1424689834237427627|\n",
    "|stairsup|nexus4|1424687988392|1424689834438660537|\n",
    "|stairsup|nexus4|1424687988592|1424689834640076553|\n",
    "|stairsup|nexus4|1424687988794|1424689834841675674|\n",
    "|stairsup|nexus4|1424687988999|1424689835047943984|\n",
    "|stairsup|nexus4|1424687989200|1424687987205721701|\n",
    "|stairsup|nexus4|1424687989409|1424689835458070221|\n",
    "|stairsup|nexus4|1424687989606|1424687987613772238|\n",
    "+--------+------+-------------+-------------------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ceba007f-51de-43d2-88ea-b8cf747304a9",
   "metadata": {
    "id": "ceba007f-51de-43d2-88ea-b8cf747304a9"
   },
   "outputs": [],
   "source": [
    "simpleTransform.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2372dd0-7d4a-4eca-9498-742d39afea56",
   "metadata": {
    "id": "e2372dd0-7d4a-4eca-9498-742d39afea56"
   },
   "source": [
    "### 2.2 Aggregations\n",
    "The following example uses the aggregation **cube** on the phone model and activity and the average x, y, z accelerations of the sensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "50886c31-3fa7-40b1-8106-f4c663d79e07",
   "metadata": {
    "id": "50886c31-3fa7-40b1-8106-f4c663d79e07",
    "outputId": "138adb14-2725-4a2b-9ed3-0bbf43b0502f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/12 16:22:59 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-82434947-5988-491e-8a36-7d4d9bb909ce. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/06/12 16:22:59 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "deviceModelStats = streaming.cube(\"gt\", \"model\").avg()\\\n",
    "  .drop(\"avg(Arrival_time)\")\\\n",
    "  .drop(\"avg(Creation_Time)\")\\\n",
    "  .drop(\"avg(Index)\")\\\n",
    "  .writeStream.queryName(\"device_counts\").format(\"memory\")\\\n",
    "  .outputMode(\"complete\")\\\n",
    "  .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe861459-f221-4140-a779-b6474243dc10",
   "metadata": {
    "id": "fe861459-f221-4140-a779-b6474243dc10",
    "outputId": "73ea238c-d3d2-410b-f220-49c531aaf024"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+------+------+\n",
      "| gt|model|avg(x)|avg(y)|avg(z)|\n",
      "+---+-----+------+------+------+\n",
      "+---+-----+------+------+------+\n",
      "\n",
      "+----------+------+--------------------+--------------------+--------------------+\n",
      "|        gt| model|              avg(x)|              avg(y)|              avg(z)|\n",
      "+----------+------+--------------------+--------------------+--------------------+\n",
      "|       sit|  NULL|-5.00171701972525...| 3.29206100777474E-4|-1.91068888778222...|\n",
      "|      walk|nexus4|-0.00457872511456...|0.001398124138588...|-0.00166899850844...|\n",
      "|      walk|  NULL|-0.00457872511456...|0.001398124138588...|-0.00166899850844...|\n",
      "|  stairsup|  NULL|-0.02528056535479...|-0.01004925366732...|-0.10050482947278248|\n",
      "|     stand|  NULL|-3.24754958945150...|2.709525612972639...| 3.83496236784506E-4|\n",
      "|      bike|  NULL|0.023420025203896726|-0.00878518609104...|-0.08219914587308406|\n",
      "|  stairsup|nexus4|-0.02528056535479...|-0.01004925366732...|-0.10050482947278248|\n",
      "|      NULL|nexus4|5.367647483176713E-4|-0.00640008059352251|-0.01029803122935...|\n",
      "|      NULL|  NULL|5.367647483176713E-4|-0.00640008059352251|-0.01029803122935...|\n",
      "|stairsdown|  NULL| 0.02266317507373911|-0.03452328420065675| 0.12128234524839329|\n",
      "|      null|  NULL|-0.00845402352661...|-1.67688497973404...|0.001853631201916...|\n",
      "|       sit|nexus4|-5.00171701972525...| 3.29206100777474E-4|-1.91068888778222...|\n",
      "|stairsdown|nexus4| 0.02266317507373911|-0.03452328420065675| 0.12128234524839329|\n",
      "|     stand|nexus4|-3.24754958945150...|2.709525612972639...| 3.83496236784506E-4|\n",
      "|      null|nexus4|-0.00845402352661...|-1.67688497973404...|0.001853631201916...|\n",
      "|      bike|nexus4|0.023420025203896726|-0.00878518609104...|-0.08219914587308406|\n",
      "+----------+------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for x in range(2):\n",
    "    spark.sql(\"SELECT * FROM device_counts\").show()\n",
    "    sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b9d11a47-a40e-407c-a819-afe5bf353798",
   "metadata": {
    "id": "b9d11a47-a40e-407c-a819-afe5bf353798"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/12 16:23:05 ERROR WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 23, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@67474b9b] is aborting.\n",
      "25/06/12 16:23:05 ERROR WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 23, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@67474b9b] aborted.\n"
     ]
    }
   ],
   "source": [
    "deviceModelStats.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459bbc91-1f46-4e7a-b46c-3f3206a00866",
   "metadata": {
    "id": "459bbc91-1f46-4e7a-b46c-3f3206a00866"
   },
   "source": [
    "### 2.3 Joins\n",
    "Example for join streaming DataFrames to static DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "60ff1cef-76fd-4e15-b8af-584f087626c8",
   "metadata": {
    "id": "60ff1cef-76fd-4e15-b8af-584f087626c8",
    "outputId": "32787770-a77a-4278-fb92-fdde6151a021"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/12 16:23:06 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-629e051d-3a07-40af-8ce6-f5a1f5be5804. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/06/12 16:23:06 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "historicalAgg = activityStaticDf.groupBy(\"gt\", \"model\").avg()\n",
    "\n",
    "deviceModelStats = streaming.drop(\"Arrival_Time\", \"Creation_Time\", \"Index\")\\\n",
    "  .cube(\"gt\", \"model\").avg()\\\n",
    "  .join(historicalAgg, [\"gt\", \"model\"])\\\n",
    "  .writeStream.queryName(\"device_counts\").format(\"memory\")\\\n",
    "  .outputMode(\"complete\")\\\n",
    "  .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "46fe10a8-9aab-40cb-a4b8-644d9dc4796a",
   "metadata": {
    "id": "46fe10a8-9aab-40cb-a4b8-644d9dc4796a",
    "outputId": "bb3584fe-5c01-4416-dbed-40e5abb37bf4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+------+------+-----------------+------------------+----------+------+------+------+\n",
      "| gt|model|avg(x)|avg(y)|avg(z)|avg(Arrival_Time)|avg(Creation_Time)|avg(Index)|avg(x)|avg(y)|avg(z)|\n",
      "+---+-----+------+------+------+-----------------+------------------+----------+------+------+------+\n",
      "+---+-----+------+------+------+-----------------+------------------+----------+------+------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 266:>                (0 + 0) / 5][Stage 267:>              (0 + 16) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+------+------+-----------------+------------------+----------+------+------+------+\n",
      "| gt|model|avg(x)|avg(y)|avg(z)|avg(Arrival_Time)|avg(Creation_Time)|avg(Index)|avg(x)|avg(y)|avg(z)|\n",
      "+---+-----+------+------+------+-----------------+------------------+----------+------+------+------+\n",
      "+---+-----+------+------+------+-----------------+------------------+----------+------+------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "for x in range(2):\n",
    "    spark.sql(\"SELECT * FROM device_counts\").show()\n",
    "    sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "41941ae5-abb9-4aca-be09-5f08903d2df7",
   "metadata": {
    "id": "41941ae5-abb9-4aca-be09-5f08903d2df7",
    "outputId": "645ae454-2f5e-4c2b-ea0d-b57ff06be0fc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/12 16:23:10 ERROR WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@22854a3d] is aborting.\n",
      "25/06/12 16:23:10 ERROR WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@22854a3d] aborted.\n"
     ]
    }
   ],
   "source": [
    "deviceModelStats.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d2ec64c0-92ce-4cdb-9a37-f523e4dbdbba",
   "metadata": {
    "id": "d2ec64c0-92ce-4cdb-9a37-f523e4dbdbba",
    "outputId": "1d17671d-7fe4-4705-89c4-df70860c49c3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/12 16:23:10 WARN TaskSetManager: Lost task 11.0 in stage 269.0 (TID 1256) (192.168.76.195 executor driver): TaskKilled (Stage cancelled: Job 174 cancelled part of cancelled job group 35957687-cb64-481f-b532-ea5977e4aef1)\n",
      "25/06/12 16:23:10 WARN TaskSetManager: Lost task 3.0 in stage 269.0 (TID 1248) (192.168.76.195 executor driver): TaskKilled (Stage cancelled: Job 174 cancelled part of cancelled job group 35957687-cb64-481f-b532-ea5977e4aef1)\n",
      "25/06/12 16:23:10 WARN TaskSetManager: Lost task 9.0 in stage 269.0 (TID 1254) (192.168.76.195 executor driver): TaskKilled (Stage cancelled: Job 174 cancelled part of cancelled job group 35957687-cb64-481f-b532-ea5977e4aef1)\n",
      "25/06/12 16:23:10 WARN TaskSetManager: Lost task 1.0 in stage 269.0 (TID 1246) (192.168.76.195 executor driver): TaskKilled (Stage cancelled: Job 174 cancelled part of cancelled job group 35957687-cb64-481f-b532-ea5977e4aef1)\n",
      "25/06/12 16:23:10 WARN TaskSetManager: Lost task 5.0 in stage 269.0 (TID 1250) (192.168.76.195 executor driver): TaskKilled (Stage cancelled: Job 174 cancelled part of cancelled job group 35957687-cb64-481f-b532-ea5977e4aef1)\n",
      "25/06/12 16:23:10 WARN TaskSetManager: Lost task 14.0 in stage 269.0 (TID 1259) (192.168.76.195 executor driver): TaskKilled (Stage cancelled: Job 174 cancelled part of cancelled job group 35957687-cb64-481f-b532-ea5977e4aef1)\n",
      "25/06/12 16:23:10 WARN TaskSetManager: Lost task 7.0 in stage 269.0 (TID 1252) (192.168.76.195 executor driver): TaskKilled (Stage cancelled: Job 174 cancelled part of cancelled job group 35957687-cb64-481f-b532-ea5977e4aef1)\n",
      "25/06/12 16:23:10 WARN TaskSetManager: Lost task 12.0 in stage 269.0 (TID 1257) (192.168.76.195 executor driver): TaskKilled (Stage cancelled: Job 174 cancelled part of cancelled job group 35957687-cb64-481f-b532-ea5977e4aef1)\n",
      "25/06/12 16:23:10 WARN TaskSetManager: Lost task 2.0 in stage 269.0 (TID 1247) (192.168.76.195 executor driver): TaskKilled (Stage cancelled: Job 174 cancelled part of cancelled job group 35957687-cb64-481f-b532-ea5977e4aef1)\n",
      "25/06/12 16:23:10 WARN TaskSetManager: Lost task 15.0 in stage 269.0 (TID 1260) (192.168.76.195 executor driver): TaskKilled (Stage cancelled: Job 174 cancelled part of cancelled job group 35957687-cb64-481f-b532-ea5977e4aef1)\n",
      "25/06/12 16:23:10 WARN TaskSetManager: Lost task 0.0 in stage 269.0 (TID 1245) (192.168.76.195 executor driver): TaskKilled (Stage cancelled: Job 174 cancelled part of cancelled job group 35957687-cb64-481f-b532-ea5977e4aef1)\n",
      "25/06/12 16:23:10 WARN TaskSetManager: Lost task 4.0 in stage 269.0 (TID 1249) (192.168.76.195 executor driver): TaskKilled (Stage cancelled: Job 174 cancelled part of cancelled job group 35957687-cb64-481f-b532-ea5977e4aef1)\n",
      "25/06/12 16:23:10 WARN TaskSetManager: Lost task 8.0 in stage 269.0 (TID 1253) (192.168.76.195 executor driver): TaskKilled (Stage cancelled: Job 174 cancelled part of cancelled job group 35957687-cb64-481f-b532-ea5977e4aef1)\n",
      "25/06/12 16:23:10 WARN TaskSetManager: Lost task 10.0 in stage 269.0 (TID 1255) (192.168.76.195 executor driver): TaskKilled (Stage cancelled: Job 174 cancelled part of cancelled job group 35957687-cb64-481f-b532-ea5977e4aef1)\n",
      "25/06/12 16:23:10 WARN TaskSetManager: Lost task 6.0 in stage 269.0 (TID 1251) (192.168.76.195 executor driver): TaskKilled (Stage cancelled: Job 174 cancelled part of cancelled job group 35957687-cb64-481f-b532-ea5977e4aef1)\n",
      "25/06/12 16:23:10 WARN TaskSetManager: Lost task 13.0 in stage 269.0 (TID 1258) (192.168.76.195 executor driver): TaskKilled (Stage cancelled: Job 174 cancelled part of cancelled job group 35957687-cb64-481f-b532-ea5977e4aef1)\n"
     ]
    }
   ],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "de-venv",
   "language": "python",
   "name": "de-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
