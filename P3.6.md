# Practical 3.6: Hive

0. Launch the Ubuntu-xx.xx distribution using PowerShell. Subsequently, log in as the user hduser and start Hadoop-related Services using the hduser account. 

   Note: The ampersand & starts the service in the background. Therefore, press enter before proceeding to the next step.

   ~~~bash
   hduser@MyPC:~$ cd ~
   hduser@MyPC:~$ cd ~/hadoop3 
   hduser@MyPC:~/hadoop3$ sbin/start-dfs.sh 
   hduser@MyPC:~/hadoop3$ sbin/start-yarn.sh
   hduser@MyPC:~/hadoop3$ cd ~/kafka
   hduser@MyPC:~/kafka$ bin/zookeeper-server-start.sh config/zookeeper.properties &
   hduser@MyPC:~/kafka$ bin/kafka-server-start.sh config/server.properties &
   hduser@MyPC:~/kafka$ cd ~/hbase
   hduser@MyPC:~/hbase$ bin/start-hbase.sh
   ~~~
   > Ensure that you have done the following commands before proceed.
   >
   > ~~~bash
   > $ hdfs dfs -mkdir /tmp
   > $ hdfs dfs -mkdir /tmp/hive
   > $ hdfs dfs -mkdir /user
   > $ hdfs dfs -mkdir /user/hive
   > $ hdfs dfs -mkdir /user/hive/warehouse
   > $ hdfs dfs -mkdir /user/hive/lib
   > $ hdfs dfs -chmod g+w /tmp
   > $ hdfs dfs -chmod g+w /tmp/hive
   > $ hdfs dfs -chmod g+w /user
   > $ hdfs dfs -chmod g+w /user/hive
   > $ hdfs dfs -chmod g+w /user/hive/warehouse
   > $ hdfs dfs -chmod 777 /user/hive/lib
   > ~~~

1. Configure and Start Hive-Related Services

   1.1 Start Derby using hduser account

     Note: By default, the databases will be created in the current directory. Therefore, we need to change directory to targeted directory, i.e., ~/derby/data, to perform the following commands.
     ~~~bash
     hduser@MyPC:~$ cd ~/derby/data
     hduser@MyPC:~/derby/data$ nohup ~/derby/bin/startNetworkServer -h 0.0.0.0 &
     ~~~

   1.2 IMPORTANT: Edit the hadoop-env.sh file:
    ~~~bash
    hduser@MyPC:~$ nano ~/hadoop3/etc/hadoop/hadoop-env.sh
    ~~~
    > **Uncomment the last line in the file.**
    > 
    > Note: This sets the HADOOP_CLASSPATH environment variable as follows:
    > 
    > export HADOOP_CLASSPATH=/usr/lib/jvm/java-8-openjdk-amd64/lib/tools.jar:/home/hduser/hive/lib/:/home/hduser/hadoop3/share/hadoop/common/:/home/hduser/hadoop3/share/hadoop/common/lib/:/home/hduser/hadoop3/share/hadoop/client/

   1.3 Run HiveServer2
    ~~~bash
    hduser@MyPC:~$ cd ~/hive
    hduser@MyPC:~/hive$ bin/hiveserver2
    ~~~
    > Note:	Leave the session running and DO NOT CLOSE the terminal after carrying out the above command.



2. Start Beeline CLI and Connect to the HiveServer. Beeline which is a Hive client to carry out CLI commands:
   ~~~bash
   hduser@MyPC:~$ cd ~/hive
   hduser@MyPC:~/hive$ bin/beeline
   Beeline version 2.3.9 by Apache Hive
   beeline> !connect jdbc:hive2://
   Connecting to jdbc:hive2://
   Enter username for jdbc:hive2://: APP
   Enter password for jdbc:hive2://: mine
   Connected to: Apache Hive (version 2.3.9)
   Driver: Hive JDBC (version 2.3.9)
   Transaction isolation: TRANSACTION_REPEATABLE_READ
   0: jdbc:hive2://>
   ~~~
   > Note: The default username is APP while the default password is mine.




3. Run Hive CLI Commands

   3.1 List all Hive databases
   ~~~bash
   0: jdbc:hive2://> show databases;
   ~~~

   3.2 Run HDFS commands from the beeline prompt
      For example, to list the files in the data folder in our HDFS:
      ~~~bash
      0: jdbc:hive2://> dfs -ls data;
      ~~~

   3.3 Create a new database
      ~~~bash
      0: jdbc:hive2://> CREATE DATABASE sales_db;
      0: jdbc:hive2://> SHOW DATABASES;
      ~~~
      > Check Hive’s warehouse folder in HDFS, and observe the created database.
      > ~~~bash
      > 0: jdbc:hive2://> dfs -ls /user/hive/warehouse;
      > ~~~

    3.4 Switch to the new database
      ~~~bash
      0: jdbc:hive2://> USE sales_db;
      ~~~

    3.5 Create Hive tables
      ~~~bash
      0: jdbc:hive2://> SHOW TABLES;
      ~~~

      i. Create a table named “pokes” with two columns, the first being an integer and the other a string:
      ~~~bash
      0: jdbc:hive2://> CREATE TABLE pokes (foo INT, bar STRING);
      ~~~
      
      ii. Create a table named “invites'' with two columns and a partition column called ds. The partition column is a virtual column. It is not part of the data itself but is derived from the partition that a particular dataset is loaded into.
      ~~~bash
      0: jdbc:hive2://> CREATE TABLE invites (foo INT, bar STRING) PARTITIONED BY (ds STRING);
      0: jdbc:hive2://> SHOW TABLES;
      ~~~

      iii. Create a table stored as a text file:
      > CREATE TABLE Sales(
      > ID INT,
      > DESCRIPTION STRING,
      > UNIT_PRICE DOUBLE,
      > QUANTITY INT
      > )
      > COMMENT 'This is the Sales table stored as textfile'
      > ROW FORMAT DELIMITED
      > FIELDS TERMINATED BY '\t'
      > STORED AS TEXTFILE;

      iv. Overwrite the data in the Sales table with the data in a text or CSV file. Here we assume that the sales1.txt file is in your HDFS’ folder named data:
      ~~~bash
      0: jdbc:hive2://> LOAD DATA INPATH 'data/sales1.txt' OVERWRITE INTO TABLE Sales;
      0: jdbc:hive2://> SELECT * from Sales;
      ~~~

      v. Overwrite the data in the Sales table with the data in a text or CSV file. Here we assume that the sales2.txt file is in your HDFS’ folder named data:
      ~~~bash
      0: jdbc:hive2://> LOAD DATA INPATH 'data/sales2.txt' INTO TABLE Sales;
      0: jdbc:hive2://> SELECT * from Sales;
      ~~~

    3.6 Drop tables and databases

    i. Drop a tables
      ~~~bash
      0: jdbc:hive2://> DROP TABLE <table_name>;
      ~~~

      ii. Drop a database:
      ~~~bash
      0: jdbc:hive2://> DROP DATABASE <database_name> CASCADE;
      ~~~

      iii. Display Hive’s warehouse location:
      ~~~bash
      0: jdbc:hive2://> set hive.metastore.warehouse.dir;
      ~~~

    3.7 To quit Beeline,
      ~~~bash
      0: jdbc:hive2://> !q
      ~~~



4. Assessing Hive using PySpark

   4.1 Change to the user student before start the JupyterLab service, and then access it in your browser
      ~~~bash
      hduser@MyPC:~$ su - student
      hduser@MyPC:~$ cd ~
      student@MyPC:~$ source de-prj/de-venv/bin/activate
      (de-venv) student@MyPC:~$ jupyter lab
      ~~~

   4.2 Upload the downloaded NOTEBOOK 3.6 PySpark and Hive.ipynb.
   
   4.3 Review and then run the code in the created virtual environment, i.e., de-venv.

   4.4 Deactivate the virtual environment once you no longer need to work in the environment.
      ~~~bash
      (de-venv) student@MyPC:~$ deactivate
      student@MyPC:~$ 
      ~~~

5. After done all your exercises, **stop the Hive-Related Services** with command as follows.

   5.1 Stop HiveServer2:
     At the terminal running HiveServer2, which has completed in step 1.3 with user account hduser, type **Ctrl-C to terminate Hive’s RunJar**.
     > ATTENTION: **Remember to revert back the comment, as per the step 1.2 above (refer to file ~/hadoop3/etc/hadoop/hadoop-env.sh )**

   5.2 Stop the Derby network server 
     Check the process ID of the NetworkServerControl and then, kill the process. E.g., 
     ~~~bash
     hduser@MyPC:~/derby/data$ jps                           
     . . .                                                                         
     8629 NetworkServerControl                               
     hduser@MyPC:~/derby/data$ kill -9 8629 
     ~~~

   5.3 Stop HBase
     ~~~bash
     hduser@MyPC:~$ cd ~/hbase
     hduser@MyPC:~/hbase$ bin/stop-hbase.sh
     ~~~
     
   5.4 Stop Kafka
     ~~~bash
     hduser@MyPC:~$ cd ~/kafka
     hduser@MyPC:~/kafka$ bin/kafka-server-stop.sh
     ~~~
     > Note: Wait for about 30 seconds before performing the next step.

    5.5 StopZookeeper
     ~~~bash
     hduser@MyPC:~/kafka$ bin/zookeeper-server-stop.sh
     ~~~
     > Note: Wait for about 30 seconds before performing the next step.

    5.6 Stop YARN and HDFS.
     ~~~bash
     hduser@MyPC:~$ cd ~/hadoop3
     hduser@MyPC:~/hadoop3$ sbin/stop-yarn.sh
     hduser@MyPC:~/hadoop3$ sbin/stop-dfs.sh
     ~~~




