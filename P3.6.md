# Practical 3.6: Hive

0. Start Hadoop-related Services using hduser account

   Note: The ampersand & starts the service in the background. Therefore, press enter before proceeding to the next step.

   ~~~bash
   hduser@MyPC:~$ hadoop3/sbin/start-dfs.sh 
   hduser@MyPC:~$ hadoop3/sbin/start-yarn.sh
   hduser@MyPC:~$ cd ~/kafka
   hduser@MyPC:~/kafka$ bin/zookeeper-server-start.sh config/zookeeper.properties &
   hduser@MyPC:~/kafka$ bin/kafka-server-start.sh config/server.properties &
   hduser@MyPC:~$ cd ~/hbase
   hduser@MyPC:~/hbase$ bin/start-hbase.sh
   ~~~

1. Configure and Start Hive-Related Services

   1.1 Start Derby using hduser account

     Note: By default, the databases will be created in the current directory.
     ~~~bash
     hduser@MyPC:~$ cd ~/derby/data
     hduser@MyPC:~/derby/data$ nohup ~/derby/bin/startNetworkServer -h 0.0.0.0 &
     ~~~

   1.2 Edit the hadoop-env.sh file:
    ~~~bash
    hduser@MyPC:~$ nano ~/hadoop3/etc/hadoop/hadoop-env.sh
    ~~~
    > Uncomment the last line in the file.
    > 
    > Note: This sets the HADOOP_CLASSPATH environment variable as follows:
    > 
    > export HADOOP_CLASSPATH=/usr/lib/jvm/java-8-openjdk-amd64/lib/tools.jar:/home/hduser/hive/lib/:/home/hduser/hadoop3/share/hadoop/common/:/home/hduser/hadoop3/share/hadoop/common/lib/:/home/hduser/hadoop3/share/hadoop/client/

   1.3 Run HiveServer2
    ~~~bash
    hduser@MyPC:~$ cd ~/hive
    hduser@MyPC:~/hive$ bin/hiveserver2
    ~~~
    > Note:	Leave the session running and DO NOT CLOSE the terminal after carrying out the above command.



2. Start Beeline CLI and Connect to the HiveServer

   Start WSL in another terminal to carry out the following.

   2.1 Start beeline which is a Hive client to carry out CLI commands:
   ~~~bash
   hduser@MyPC:~$ cd ~/hive
   hduser@MyPC:~$ bin/beeline
   Beeline version 2.3.9 by Apache Hive
   beeline> !connect jdbc:hive2://
   Connecting to jdbc:hive2://
   Enter username for jdbc:hive2://: APP
   Enter password for jdbc:hive2://: mine
   Connected to: Apache Hive (version 2.3.9)
   Driver: Hive JDBC (version 2.3.9)
   Transaction isolation: TRANSACTION_REPEATABLE_READ
   0: jdbc:hive2://>
   ~~~

   2.2 Connect to HiveServer2

   Note: The default username is APP while the default password is mine.
   ~~~bash
   beeline> !connect jdbc:hive2://
   Connecting to jdbc:hive2://
   Enter username for jdbc:hive2://: APP
   Enter password for jdbc:hive2://: ****
   Hive Session ID = 79236233-217b-4065-ac10-8a2274ca5eb5
   23/01/30 14:21:39 [main]: WARN session.SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
   Connected to: Apache Hive (version 3.1.2)
   Driver: Hive JDBC (version 3.1.2)
   Transaction isolation: TRANSACTION_REPEATABLE_READ
   0: jdbc:hive2://>
   ~~~



3. Run Hive CLI Commands

   3.1 List all Hive databases
   ~~~bash
   0: jdbc:hive2://> show databases;
   ~~~

   3.2 Run HDFS commands from the beeline prompt
      For example, to list the files in the data folder in our HDFS:
      ~~~bash
      0: jdbc:hive2://> dfs -ls data;
      ~~~

   3.3 Create a new database
      ~~~bash
      0: jdbc:hive2://> CREATE DATABASE sales_db;
      0: jdbc:hive2://> SHOW DATABASES;
      ~~~
      > Check Hive’s warehouse folder in HDFS, and observe the created database.
      > ~~~bash
      > 0: jdbc:hive2://> dfs -ls /user/hive/warehouse;
      > ~~~

    3.4 Switch to the new database
      ~~~bash
      0: jdbc:hive2://> USE sales_db;
      ~~~

    3.5 Create Hive tables
      ~~~bash
      0: jdbc:hive2://> SHOW TABLES;
      ~~~

       i. Create a table named “pokes” with two columns, the first being an integer and the other a string:
         ~~~bash
         0: jdbc:hive2://> CREATE TABLE pokes (foo INT, bar STRING);
         ~~~
      
Create a table named “invites'' with two columns and a partition column called ds. The partition column is a virtual column. It is not part of the data itself but is derived from the partition that a particular dataset is loaded into.
0: jdbc:hive2://> CREATE TABLE invites (foo INT, bar STRING) PARTITIONED BY (ds STRING);

0: jdbc:hive2://> SHOW TABLES;
+-----------+
| tab_name  |
+-----------+
| invites   |
| pokes 	|
+-----------+


Create a table stored as a text file:
CREATE TABLE Sales(
ID INT,
DESCRIPTION STRING,
UNIT_PRICE DOUBLE,
QUANTITY INT
)
COMMENT 'This is the Sales table stored as textfile'
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\t'
STORED AS TEXTFILE;

Overwrite the data in the Sales table with the data in a text or CSV file. Here we assume that the sales1.txt file is in your HDFS’ folder named data:
0: jdbc:hive2://> LOAD DATA INPATH 'data/sales1.txt' OVERWRITE INTO TABLE Sales;
Loading data to table salesdb.sales
OK
No rows affected (0.337 seconds)

0: jdbc:hive2://> SELECT * from Sales;
OK
+-----------+--------------------+-------------------+-----------------+
| sales.id  | sales.description  | sales.unit_price  | sales.quantity  |
+-----------+--------------------+-------------------+-----------------+
| 1005  	| pen            	| 2.5           	| 4           	|
| 1007  	| pencil         	| 1.0           	| 10          	|
| 1001  	| notebook       	| 5.0           	| 2           	|
| 1003  	| ruler          	| 1.0           	| 1           	|
| 1002  	| calculator     	| 55.0          	| 1           	|
+-----------+--------------------+-------------------+-----------------+
6 rows selected (0.825 seconds)


Overwrite the data in the Sales table with the data in a text or CSV file. Here we assume that the sales2.txt file is in your HDFS’ folder named data:
0: jdbc:hive2://> LOAD DATA INPATH 'data/sales2.txt' INTO TABLE Sales;

0: jdbc:hive2://> SELECT * from Sales;
OK
+-----------+--------------------+-------------------+-----------------+
| sales.id  | sales.description  | sales.unit_price  | sales.quantity  |
+-----------+--------------------+-------------------+-----------------+
| 1005  	| pen            	| 2.5           	| 4           	|
| 1007  	| pencil         	| 1.0           	| 10          	|
| 1001  	| notebook       	| 5.0           	| 2           	|
| 1003  	| ruler          	| 1.0           	| 1           	|
| 1002  	| calculator     	| 55.0          	| 1           	|
| 2005  	| A4paper        	| 7.8           	| 2           	|
| 2007  	| eraser         	| 2.0           	| 4           	|
| 2001  	| watercolours   	| 12.5          	| 1           	|
| 2003  	| paintbrush     	| 3.0           	| 4           	|
+-----------+--------------------+-------------------+-----------------+


4.6  Drop tables and databases
Drop a tables
0: jdbc:hive2://> DROP TABLE <table_name>;
Drop a database:
0: jdbc:hive2://> DROP DATABASE <database_name> CASCADE;
Display Hive’s warehouse location:
0: jdbc:hive2://> set hive.metastore.warehouse.dir;
+----------------------------------------------------+
|                    	set                     	|
+----------------------------------------------------+
| hive.metastore.warehouse.dir=/user/hive/warehouse  |
+----------------------------------------------------+
1 row selected (0.003 seconds)

4.6  To quit Beeline,
0: jdbc:hive2://> !q
Closing: 0: jdbc:hive2://



Assessing Hive using PySpark

Create a new directory named hive-code.

Upload NOTEBOOK 5.6 PySpark and Hive. to the newly created folder.

Change directory to the test_hive directory.

Launch Jupyter Notebook:
hduser@PC25:~/test_hive$ jupyter notebook --port=8888 --no-browser

Review and then run the code.

To Stop Hive-Related Services

6.1 Stop HiveServer2:
At the terminal running HiveServer2, type Ctrl-C to terminate Hive’s RunJar.

6.2 Stop the Derby network server 
Check the process ID of the NetworkServerControl and then, kill the process. E.g., 
hduser@PC25:~/derby/data$ jps                           
. . .                                                                         
8629 NetworkServerControl                               
hduser@PC25:~/derby/data$ kill -9 8629 

6.3 Stop HBase
hduser@PC25:~$ cd ~/hbase
hduser@PC25:~/hbase$ bin/stop-hbase.sh

6.4 Stop Kafka

hduser@PC25:~$ cd ~/kafka
hduser@PC25:~/kafka$ bin/zookeeper-server-stop.sh
Note: Wait for about 30 seconds before performing the next step.

6.5 StopZookeeper

hduser@PC25:~/kafka$ bin/zookeeper-server-stop.sh
Note: Wait for about 30 seconds before performing the next step.

6.6 Stop YARN and HDFS.

hduser@PC25:~$ hadoop3/sbin/stop-yarn.sh
hduser@PC25:~$ hadoop3/sbin/stop-dfs.sh





