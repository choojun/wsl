# Practical 3.6: Hive

0. Start Hadoop-related Services using hduser account

   Note: The ampersand & starts the service in the background. Therefore, press enter before proceeding to the next step.

   ~~~bash
   hduser@MyPC:~$ cd ~
   hduser@MyPC:~$ cd ~/hadoop3 
   hduser@MyPC:~/hadoop3$ sbin/start-dfs.sh 
   hduser@MyPC:~/hadoop3$ sbin/start-yarn.sh
   hduser@MyPC:~/hadoop3$ cd ~/kafka
   hduser@MyPC:~/kafka$ bin/zookeeper-server-start.sh config/zookeeper.properties &
   hduser@MyPC:~/kafka$ bin/kafka-server-start.sh config/server.properties &
   hduser@MyPC:~/kafka$ cd ~/hbase
   hduser@MyPC:~/hbase$ bin/start-hbase.sh
   ~~~
   > Ensure that you have done the following commands before proceed.
   >
   > ~~~bash
   > $ hdfs dfs -mkdir /tmp
   > $ hdfs dfs -mkdir /tmp/hduser
   > $ hdfs dfs -mkdir /user
   > $ hdfs dfs -mkdir /user/hduser
   > $ hdfs dfs -mkdir /user/hduser/warehouse
   > $ hdfs dfs -mkdir /user/hduser/lib
   > $ hdfs dfs -chmod g+w /tmp
   > $ hdfs dfs -chmod g+w /tmp/hduser
   > $ hdfs dfs -chmod g+w /user
   > $ hdfs dfs -chmod g+w /user/hduser
   > $ hdfs dfs -chmod g+w /user/hduser/warehouse
   > $ hdfs dfs -chmod 777 /user/hduser/lib
   > ~~~

1. Configure and Start Hive-Related Services

   1.1 Start Derby using hduser account

     Note: By default, the databases will be created in the current directory. Therefore, we need to change directory to targeted directory, i.e., ~/derby/data, to perform the following commands.
     ~~~bash
     hduser@MyPC:~$ cd ~/derby/data
     hduser@MyPC:~/derby/data$ nohup ~/derby/bin/startNetworkServer -h 0.0.0.0 &
     ~~~

   1.2 IMPORTANT: Edit the hadoop-env.sh file:
    ~~~bash
    hduser@MyPC:~$ nano ~/hadoop3/etc/hadoop/hadoop-env.sh
    ~~~
    > **Uncomment the last line in the file.**
    > 
    > Note: This sets the HADOOP_CLASSPATH environment variable as follows:
    > 
    > export HADOOP_CLASSPATH=/usr/lib/jvm/java-8-openjdk-amd64/lib/tools.jar:/home/hduser/hive/lib/:/home/hduser/hadoop3/share/hadoop/common/:/home/hduser/hadoop3/share/hadoop/common/lib/:/home/hduser/hadoop3/share/hadoop/client/

   1.3 Run HiveServer2
    ~~~bash
    hduser@MyPC:~$ cd ~/hive
    hduser@MyPC:~/hive$ bin/hiveserver2
    ~~~
    > Note:	Leave the session running and DO NOT CLOSE the terminal after carrying out the above command.



2. Start Beeline CLI and Connect to the HiveServer

   Start WSL in another terminal to carry out the following.

   2.1 Start beeline which is a Hive client to carry out CLI commands:
   ~~~bash
   hduser@MyPC:~$ cd ~/hive
   hduser@MyPC:~$ bin/beeline
   Beeline version 2.3.9 by Apache Hive
   beeline> !connect jdbc:hive2://
   Connecting to jdbc:hive2://
   Enter username for jdbc:hive2://: APP
   Enter password for jdbc:hive2://: mine
   Connected to: Apache Hive (version 2.3.9)
   Driver: Hive JDBC (version 2.3.9)
   Transaction isolation: TRANSACTION_REPEATABLE_READ
   0: jdbc:hive2://>
   ~~~

   2.2 Connect to HiveServer2

   Note: The default username is APP while the default password is mine.
   ~~~bash
   beeline> !connect jdbc:hive2://
   Connecting to jdbc:hive2://
   Enter username for jdbc:hive2://: APP
   Enter password for jdbc:hive2://: ****
   Hive Session ID = 79236233-217b-4065-ac10-8a2274ca5eb5
   23/01/30 14:21:39 [main]: WARN session.SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
   Connected to: Apache Hive (version 3.1.2)
   Driver: Hive JDBC (version 3.1.2)
   Transaction isolation: TRANSACTION_REPEATABLE_READ
   0: jdbc:hive2://>
   ~~~



3. Run Hive CLI Commands

   3.1 List all Hive databases
   ~~~bash
   0: jdbc:hive2://> show databases;
   ~~~

   3.2 Run HDFS commands from the beeline prompt
      For example, to list the files in the data folder in our HDFS:
      ~~~bash
      0: jdbc:hive2://> dfs -ls data;
      ~~~

   3.3 Create a new database
      ~~~bash
      0: jdbc:hive2://> CREATE DATABASE sales_db;
      0: jdbc:hive2://> SHOW DATABASES;
      ~~~
      > Check Hive’s warehouse folder in HDFS, and observe the created database.
      > ~~~bash
      > 0: jdbc:hive2://> dfs -ls /user/hive/warehouse;
      > ~~~

    3.4 Switch to the new database
      ~~~bash
      0: jdbc:hive2://> USE sales_db;
      ~~~

    3.5 Create Hive tables
      ~~~bash
      0: jdbc:hive2://> SHOW TABLES;
      ~~~

      i. Create a table named “pokes” with two columns, the first being an integer and the other a string:
      ~~~bash
      0: jdbc:hive2://> CREATE TABLE pokes (foo INT, bar STRING);
      ~~~
      
      ii. Create a table named “invites'' with two columns and a partition column called ds. The partition column is a virtual column. It is not part of the data itself but is derived from the partition that a particular dataset is loaded into.
      ~~~bash
      0: jdbc:hive2://> CREATE TABLE invites (foo INT, bar STRING) PARTITIONED BY (ds STRING);
      0: jdbc:hive2://> SHOW TABLES;
      ~~~

      iii. Create a table stored as a text file:
      > CREATE TABLE Sales(
      > ID INT,
      > DESCRIPTION STRING,
      > UNIT_PRICE DOUBLE,
      > QUANTITY INT
      > )
      > COMMENT 'This is the Sales table stored as textfile'
      > ROW FORMAT DELIMITED
      > FIELDS TERMINATED BY '\t'
      > STORED AS TEXTFILE;

      iv. Overwrite the data in the Sales table with the data in a text or CSV file. Here we assume that the sales1.txt file is in your HDFS’ folder named data:
      ~~~bash
      0: jdbc:hive2://> LOAD DATA INPATH 'data/sales1.txt' OVERWRITE INTO TABLE Sales;
      0: jdbc:hive2://> SELECT * from Sales;
      ~~~

      v. Overwrite the data in the Sales table with the data in a text or CSV file. Here we assume that the sales2.txt file is in your HDFS’ folder named data:
      ~~~bash
      0: jdbc:hive2://> LOAD DATA INPATH 'data/sales2.txt' INTO TABLE Sales;
      0: jdbc:hive2://> SELECT * from Sales;
      ~~~

    3.6 Drop tables and databases

    i. Drop a tables
      ~~~bash
      0: jdbc:hive2://> DROP TABLE <table_name>;
      ~~~

      ii. Drop a database:
      ~~~bash
      0: jdbc:hive2://> DROP DATABASE <database_name> CASCADE;
      ~~~

      iii. Display Hive’s warehouse location:
      ~~~bash
      0: jdbc:hive2://> set hive.metastore.warehouse.dir;
      ~~~

    3.7 To quit Beeline,
      ~~~bash
      0: jdbc:hive2://> !q
      ~~~



4. Assessing Hive using PySpark

   4.1 Create a new directory named hive-code.

   4.2 Upload NOTEBOOK 3.6 PySpark and Hive.ipynb to the newly created folder.

   4.3 Change directory to the test_hive directory.

   4.4 Launch Jupyter Notebook:
      ~~~bash
      hduser@MyPC:~/test_hive$ jupyter notebook --port=8888 --no-browser
      ~~~

   4.5 Review and then run the code.


5. To Stop Hive-Related Services

   5.1 Stop HiveServer2:
     At the terminal running HiveServer2, type Ctrl-C to terminate Hive’s RunJar.
     > ATTENTION: **Remember to revert back the comment, as per the step 1.2 above (refer to file ~/hadoop3/etc/hadoop/hadoop-env.sh )**

   5.2 Stop the Derby network server 
     Check the process ID of the NetworkServerControl and then, kill the process. E.g., 
     ~~~bash
     hduser@MyPC:~/derby/data$ jps                           
     . . .                                                                         
     8629 NetworkServerControl                               
     hduser@MyPC:~/derby/data$ kill -9 8629 
     ~~~

   5.3 Stop HBase
     ~~~bash
     hduser@MyPC:~$ cd ~/hbase
     hduser@MyPC:~/hbase$ bin/stop-hbase.sh
     ~~~
     
   5.4 Stop Kafka
     ~~~bash
     hduser@MyPC:~$ cd ~/kafka
     hduser@MyPC:~/kafka$ bin/kafka-server-stop.sh
     ~~~
     > Note: Wait for about 30 seconds before performing the next step.

    5.5 StopZookeeper
     ~~~bash
     hduser@MyPC:~/kafka$ bin/zookeeper-server-stop.sh
     ~~~
     > Note: Wait for about 30 seconds before performing the next step.

    5.6 Stop YARN and HDFS.
     ~~~bash
     hduser@MyPC:~$ cd ~/hadoop3
     hduser@MyPC:~/hadoop3$ sbin/stop-yarn.sh
     hduser@MyPC:~/hadoop3$ sbin/stop-dfs.sh
     ~~~




