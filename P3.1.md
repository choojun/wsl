# PRACTICAL 3.1: PySpark Shell

0. Login as the user hduser, and start up HDFS and YARN. 

1. Login as the user student.

   1.1 Add Spark-related variables to studentâ€™s profile using command, i.e. nano ~/.profile, and add the following environment variables to the end of the file.
   ~~~bash
   export SPARK_HOME=/home/hduser/spark
   export PATH=$PATH:$SPARK_HOME/bin
   ~~~

   1.2 Source the ~/.profile file
   ~~~bash
   student@MyPC:~$ source ~/.profile
   ~~~
   
2. Using the PySpark Interactive Shell: Word count example

  2.1 Launch the PySpark interactive shell
   ~~~bash
   $ pyspark
   ~~~




Create an RDD with data from a text file: 
Note: The shakespeare.txt was already downloaded in Practical 2. 
>>> text = sc.textFile("shakespeare.txt")
>>> print(text)



Transform the RDD to implement the word count application using Spark: 
>>> from operator import add
>>> def tokenize(text):
...     return text.split()
...
>>> words = text.flatMap(tokenize)

Apply the map:
>>> wc = words.map(lambda x: (x, 1))
>>> print(wc.toDebugString())



Apply the reduceByKey action to obtain the word counts and save the results in a text file: 
>>> counts = wc.reduceByKey(add)
   >>> counts.saveAsTextFile("wc")
Note:	To explicitly indicate the HDFS path, use "hdfs://localhost:9000/user/student/wc" as the output file name:


Exit the PySpark interactive shell
>>> exit()

Check the HDFS current working directory for the newly created directoryâ€™s contents:
student@PC25:~$ hdfs dfs -ls wc


Use the head command to view one of the part files: 
$ hdfs dfs -head wc/part-00000




ã€°ã€°ã€°ã€°ã€°ã€° End of Practical ã€°ã€°ã€°ã€°ã€°ã€°

ðŸ’¡ If no more activity using Spark or Hadoop, remember to stop the YARN service followed by the HDFS service.


