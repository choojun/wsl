# PRACTICAL 3.1: PySpark Shell

0. Login as the user hduser, and start up HDFS and YARN. 

1. Login as the user student.

   1.1 Add Spark-related variables to student’s profile using command, i.e. nano ~/.profile, and add the following environment variables (IF NOT EXIST) to the end of the file.
   ~~~bash
   export SPARK_HOME=/home/hduser/spark
   export PATH=$PATH:$SPARK_HOME/bin
   ~~~

   1.2 Source the ~/.profile file
   ~~~bash
   student@MyPC:~$ source ~/.profile
   ~~~
   
2. Using the PySpark Interactive Shell: Word count example

   2.1 Launch the PySpark interactive shell
      ~~~bash
      $ pyspark
      ~~~

   2.2 Create an RDD with data from a text file: 
      ~~~bash
      >>> text = sc.textFile("shakespeare.txt")
      >>> print(text)
      ~~~
      > Note: The shakespeare.txt was already downloaded in Practical 2.


   2.3 Transform the RDD to implement the word count application using Spark: 
      ~~~bash
      >>> from operator import add
      >>> def tokenize(text):
      ...     return text.split()
      ...
      >>> words = text.flatMap(tokenize)
      ~~~

   2.4 Apply the map:
      ~~~bash
      >>> wc = words.map(lambda x: (x, 1))
      >>> print(wc.toDebugString())
      ~~~


   2.5 Apply the reduceByKey action to obtain the word counts and save the results in a text file: 
      ~~~bash
      >>> counts = wc.reduceByKey(add)
      >>> counts.saveAsTextFile("wc")
      ~~~
      > Note:	To explicitly indicate the HDFS path, use "hdfs://localhost:9000/user/student/wc" as the output file name:

    2.6 Exit the PySpark interactive shell
      ~~~bash
      >>> exit()
      ~~~

    2.7 Check the HDFS current working directory for the newly created directory’s contents:
      ~~~bash
      student@MyPC:~$ hdfs dfs -ls wc
      ~~~

    2.8 Use the head command to view one of the part files: 
      ~~~bash
      student@MyPC:~$ hdfs dfs -head wc/part-00000
      ~~~



# End of Practical

If no more activity using Spark or Hadoop, remember to stop the YARN service followed by the HDFS service.


