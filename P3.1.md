# PRACTICAL 3.1: PySpark Resilient Distributed Datasets (RDDs)

0. Launch the setup Ubuntu-xx.xx distro using PowerShell. Subsequently, log in as the user hduser and start HDFS and YARN services.

1. Login as the user student, and put data files in HDFS

   1.1 Download/copy the de folder from the course’s Google Classroom to your C:\ drive.

   1.2 Copy the de folder from Windows to WSL:
      ~~~bash
      student@MyPC:~$ cp -r /mnt/c/Users/choojun/Documents/de /home/student
      ~~~
      > In case you downloaded the de folder with missing data sub-folder initially, and then subsequently downloaded the data folder into the de folder:
      > ~~~bash
      > student@MyPC:~$ cp -r /mnt/c/Users/choojun/Documents/de/data /home/student/de/
      > ~~~

    1.3 Put the de/data folder in HDFS (this may take some time):
      ~~~bash
      student@MyPC:~$ hdfs dfs -put /home/student/de/data .
      ~~~

2. Access JupyterLab

   2.1 Start the JupyterLab service
   > Refer to steps 6.* of P0.

   2.2 Access JupyterLab in your browser

3. PySpark RDD

   3.1 Open the Jupyter notebook NOTEBOOK 3.1 Spark RDDs.ipynb in your JupyterLab’s de directory.

   3.2 For each cell, i) review and understand the code, and ii) run the code and observe its output.
   > In file .profile of user student account, you need to export your $PYSPARK_PYTHON with your virtualenv, i.e., export PYSPARK_PYTHON={path/to/your/virtualenv}/bin/python
   > 
   > Example:
   > 
   > export PYSPARK_PYTHON=/home/student/de-prj/de-venv/bin/python

4. More exercixes on PySpark RDD

   4.1 Write PySpark code for the following past years question 202405: Question 4
   > If you have carried out the Pre-Practical Steps, the data.txt file should already be in HDFS.

   4.2 Write PySpark code for the following past years question 202301: Question 4
   > If you have carried out the Pre-Practical Steps, the OrderInformation.txt should already be in HDFS.


# End of Practical

If no more activity using Spark or Hadoop, remember to stop the YARN service followed by the HDFS service.


