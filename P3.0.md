# PRACTICAL 3.0: PySpark Environment

0. Launch the setup Ubuntu-xx.xx distro using PowerShell. Subsequently, log in as the user hduser and start HDFS and YARN services.

1. Login as the user student.

   1.1 Add Spark-related variables to studentâ€™s profile using command, i.e. nano ~/.profile, and add the following environment variables (IF NOT EXIST) to the end of the file.
   ~~~bash
   export SPARK_HOME=/home/hduser/spark
   export PATH=$PATH:$SPARK_HOME/bin
   ~~~

   1.2 Source the ~/.profile file
   ~~~bash
   student@MyPC:~$ source ~/.profile
   ~~~
   
2. Test it using the PySpark Interactive Shell: Word count example

   2.1 Launch the PySpark interactive shell
      ~~~bash
      student@MyPC:~$ pyspark
      ~~~
      > You should be able to observe the response on screen, indicating the version of Spark installed.

   2.2 Suppose that you have copied file shakespeare.txt into directory /user/student of HDFS. Create an RDD with data from the text file, and transform the RDD to implement the word count application using Spark
      ~~~bash
      >>> text_rdd = sc.textFile("shakespeare.txt")
      >>> print(text_rdd)
      >>>
      >>> from operator import add
      >>> def tokenize(text):
      ... return text.split()
      ...
      >>> words = text.flatMap(tokenize)
      ~~~


# End of Practical

If no more activity using Spark or Hadoop, remember to stop the YARN service followed by the HDFS service.


