# PRACTICAL 3.0: PySpark Environment

0. Launch the setup Ubuntu-xx.xx distro using PowerShell. Subsequently, log in as the user hduser and start HDFS and YARN services.

1. Login as the user student.

   1.1 Add Spark-related variables to student’s profile using command, i.e. nano ~/.profile, and add the following environment variables (IF NOT EXIST) to the end of the file.
   ~~~bash
   export SPARK_HOME=/home/hduser/spark
   export PATH=$PATH:$SPARK_HOME/bin
   ~~~

   1.2 Source the ~/.profile file
   ~~~bash
   student@MyPC:~$ source ~/.profile
   ~~~
   
2. Test it using the PySpark Interactive Shell: Word count example

   2.1 Launch the PySpark interactive shell
      ~~~bash
      student@MyPC:~$ pyspark
      ~~~
      > You should be able to observe the response on screen, indicating the version of Spark installed.

   2.2 Suppose that you have copied file shakespeare.txt into directory /user/student of HDFS. Create an RDD with data from the text file, and transform the RDD to implement the word count application using Spark
      ~~~bash
      >>> text_rdd = sc.textFile("shakespeare.txt")
      >>> print(text_rdd)
      >>>
      >>> from operator import add
      >>> def tokenize(text):
      ...    return text.split()
      ...
      >>> words = text_rdd.flatMap(tokenize)
      ~~~

   2.3	Apply the map, and then apply the reduceByKey action to obtain the word counts before saving the results in file
      ~~~bash
      >>> wc = words.map(lambda x: (x, 1))
      >>> print(wc.toDebugString())
      >>> counts = wc.reduceByKey(add)
      >>> counts.saveAsTextFile("hdfs://localhost:9000/user/hduser/wc")
      >>> exit()
      ~~~
      > Apache spark version 3.3.6 does not need to have the hdfs directory path (hdfs://localhost:9000/user/hduser/) explicitly indicated to write in the local path, i.e., /home/hduser. Example:
      > counts.saveAsTextFile("wc")

   2.4 Check the HDFS current working directory for the newly created directory’s contents, and use the head command to view one of the part files
      ~~~bash
      $ hdfs dfs -ls wc
      $ hdfs dfs -head wc/part-00000
      ~~~

# End of Practical

If no more activity using Spark or Hadoop, remember to stop the YARN service followed by the HDFS service.


