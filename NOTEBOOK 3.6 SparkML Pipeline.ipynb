{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e69c4acf-372f-4c87-b7a6-85fcaf2092dc",
   "metadata": {
    "id": "e69c4acf-372f-4c87-b7a6-85fcaf2092dc"
   },
   "source": [
    "# NOTEBOOK 3.6 SparkML Pipeline\n",
    "Adapted from: examples/src/main/python/ml/pipeline_example.py\n",
    "\n",
    "### Pipeline Example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da53664-45bc-4e94-9313-ca7963896591",
   "metadata": {
    "id": "1da53664-45bc-4e94-9313-ca7963896591"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "u332d6vScPZu",
   "metadata": {
    "id": "u332d6vScPZu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e50716e-c852-4cc0-80ae-864504987acc",
   "metadata": {
    "id": "6e50716e-c852-4cc0-80ae-864504987acc",
    "outputId": "cdace636-a452-494b-b2aa-392b130b6257",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/05 10:52:08 WARN Utils: Your hostname, PC25. resolves to a loopback address: 127.0.1.1; using 192.168.76.195 instead (on interface eth0)\n",
      "24/06/05 10:52:08 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/06/05 10:52:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"PipelineExample\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5db8b3-5093-498c-b96d-7064565ee75f",
   "metadata": {
    "id": "7b5db8b3-5093-498c-b96d-7064565ee75f"
   },
   "outputs": [],
   "source": [
    "# Prepare (fake) training documents from a list of (id, text, label) tuples.\n",
    "training = spark.createDataFrame([\n",
    "    (0, \"a b c d e spark\", 1.0),\n",
    "    (1, \"b d\", 0.0),\n",
    "    (2, \"spark f g h\", 1.0),\n",
    "    (3, \"hadoop mapreduce\", 0.0)\n",
    "], [\"id\", \"text\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1840eefe-cd70-4fd3-9cc7-7bd4eadecab9",
   "metadata": {
    "id": "1840eefe-cd70-4fd3-9cc7-7bd4eadecab9"
   },
   "outputs": [],
   "source": [
    "# Configure an ML pipeline, which consists of three stages: tokenizer, hashingTF, and lr.\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.001)\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a647c2e-dded-405b-a0bd-022ffb9eb08c",
   "metadata": {
    "id": "6a647c2e-dded-405b-a0bd-022ffb9eb08c",
    "outputId": "2d124d8f-ae59-4980-81e2-dbcac0175ebf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Fit the pipeline to training documents.\n",
    "model = pipeline.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b83148-096c-47da-a8fa-d603074c98ea",
   "metadata": {
    "id": "75b83148-096c-47da-a8fa-d603074c98ea"
   },
   "outputs": [],
   "source": [
    "# Prepare (fake) test documents, which are unlabeled (id, text) tuples.\n",
    "test = spark.createDataFrame([\n",
    "    (4, \"spark i j k\"),\n",
    "    (5, \"l m n\"),\n",
    "    (6, \"spark hadoop spark\"),\n",
    "    (7, \"apache hadoop\")\n",
    "], [\"id\", \"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2203418d-0746-4cd6-870f-dbd6cc0451e8",
   "metadata": {
    "id": "2203418d-0746-4cd6-870f-dbd6cc0451e8",
    "outputId": "41c10898-1845-4a3a-b44f-35016e692e05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, spark i j k) --> prob=[0.6292098489668484,0.3707901510331516], prediction=0.0\n",
      "(5, l m n) --> prob=[0.984770006762304,0.015229993237696027], prediction=0.0\n",
      "(6, spark hadoop spark) --> prob=[0.13412348342566097,0.8658765165743391], prediction=1.0\n",
      "(7, apache hadoop) --> prob=[0.9955732114398529,0.00442678856014711], prediction=0.0\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on test documents and print columns of interest.\n",
    "prediction = model.transform(test)\n",
    "selected = prediction.select(\"id\", \"text\", \"probability\", \"prediction\")\n",
    "for row in selected.collect():\n",
    "    rid, text, prob, prediction = row\n",
    "    print(f\"({rid}, {text}) --> prob={str(prob)}, prediction={prediction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278c9224-b8b2-42ba-924f-a037372db67a",
   "metadata": {
    "id": "278c9224-b8b2-42ba-924f-a037372db67a"
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "de-venv",
   "language": "python",
   "name": "de-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
