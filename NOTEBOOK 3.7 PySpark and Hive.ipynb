{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f0c8784-a71a-45bf-890a-3c989e40c606",
   "metadata": {
    "id": "3f0c8784-a71a-45bf-890a-3c989e40c606"
   },
   "source": [
    "# NOTEBOOK 3.7 PySpark and Hive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc258bc6-40b3-4168-b0e4-71df67a1df4e",
   "metadata": {
    "id": "cc258bc6-40b3-4168-b0e4-71df67a1df4e"
   },
   "source": [
    "## 1. Create SparkSession with Hive Enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KmHqRk30epHf",
   "metadata": {
    "id": "KmHqRk30epHf"
   },
   "outputs": [],
   "source": [
    "!pip install numpy pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a6814f-3e48-45cf-86b4-def8e453a52f",
   "metadata": {
    "id": "78a6814f-3e48-45cf-86b4-def8e453a52f"
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "\n",
    "spark = SparkSession\\\n",
    "         .builder\\\n",
    "         .appName(\"SparkHiveDemo\")\\\n",
    "         .config('spark.sql.warehouse.dir', 'hdfs:/user/hive/warehouse/')\\\n",
    "         .config(\"spark.sql.catalogImplementation\", \"hive\")\\\n",
    "         .enableHiveSupport()\\\n",
    "         .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc98c60d-735e-4399-a411-1032dbb91685",
   "metadata": {
    "id": "dc98c60d-735e-4399-a411-1032dbb91685"
   },
   "source": [
    "### 1.1 Check Spark Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233018c6-bcec-4228-adca-4cde848c9908",
   "metadata": {
    "id": "233018c6-bcec-4228-adca-4cde848c9908",
    "outputId": "4851b3fc-e456-4823-a539-cbbb79a9d3f9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.5.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9637a131-7aaf-4e77-8839-06b410bc801f",
   "metadata": {
    "id": "9637a131-7aaf-4e77-8839-06b410bc801f"
   },
   "source": [
    "### 1.2 Get SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613d32e3-6f3d-4ce6-90ff-ff722fca3550",
   "metadata": {
    "id": "613d32e3-6f3d-4ce6-90ff-ff722fca3550",
    "outputId": "2a2449b9-89f3-44df-e4e2-1bd846b5d15d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.123.51.201:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>SparkHiveDemo</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=SparkHiveDemo>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = spark.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf644660-387e-45a9-8a24-0ae39d15d772",
   "metadata": {
    "id": "cf644660-387e-45a9-8a24-0ae39d15d772"
   },
   "source": [
    "### 1.3 Get Spark Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c966eaba-4c28-49a9-98e8-0e178d5aca94",
   "metadata": {
    "id": "c966eaba-4c28-49a9-98e8-0e178d5aca94"
   },
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "conf = sc.getConf()\n",
    "configurations = conf.getAll()\n",
    "pprint.pprint(configurations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b930f65-990d-4246-bfeb-da759df3464d",
   "metadata": {
    "id": "4b930f65-990d-4246-bfeb-da759df3464d"
   },
   "source": [
    "## 2. Access Existing Databases in Hive Warehouse\n",
    "\n",
    "### 2.1 List existing databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a433c4-15f2-4512-a76d-db4e73d67f99",
   "metadata": {
    "id": "e5a433c4-15f2-4512-a76d-db4e73d67f99",
    "outputId": "2d228945-620c-4dc9-8acd-75b8d991d617"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/31 11:40:38 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "|     hrdb|\n",
      "|  salesdb|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW DATABASES\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70697e3b-8f06-4ec9-9bbe-9413cdf87dda",
   "metadata": {
    "id": "70697e3b-8f06-4ec9-9bbe-9413cdf87dda"
   },
   "source": [
    "### 2.2 List existing tables\n",
    "\n",
    "(a) Show existing tables in the **default** database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919fd6c5-4d2c-4b65-a0f7-893b8fc48f46",
   "metadata": {
    "id": "919fd6c5-4d2c-4b65-a0f7-893b8fc48f46",
    "outputId": "ec2050ef-48aa-4b69-dcc6-2ca5785d8dc6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tables = spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d785880d-b799-41c4-b054-bcda60f63e97",
   "metadata": {
    "id": "d785880d-b799-41c4-b054-bcda60f63e97"
   },
   "source": [
    "(b) Show existing tables in the **salesdb** database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0883b79-8994-44e4-84a2-680745187758",
   "metadata": {
    "id": "c0883b79-8994-44e4-84a2-680745187758",
    "outputId": "168efb73-7fe1-4716-84ef-f604f218b771"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "|  salesdb|  invites|      false|\n",
      "|  salesdb|    pokes|      false|\n",
      "|  salesdb|    sales|      false|\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('USE salesdb')\n",
    "tables = spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b4cd37-8417-40e0-85b3-2100e004f24d",
   "metadata": {
    "id": "31b4cd37-8417-40e0-85b3-2100e004f24d"
   },
   "source": [
    "### 2.3 Run DML commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cbebd1-3e9c-46f8-9d9b-509b7c02ff88",
   "metadata": {
    "id": "a5cbebd1-3e9c-46f8-9d9b-509b7c02ff88",
    "outputId": "f19f9409-3dd5-43d3-8c8e-9809049a9181"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/31 11:40:42 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------+----------+--------+\n",
      "|  id| description|unit_price|quantity|\n",
      "+----+------------+----------+--------+\n",
      "|1005|         pen|       2.5|       4|\n",
      "|1007|      pencil|       1.0|      10|\n",
      "|1001|    notebook|       5.0|       2|\n",
      "|1003|       ruler|       1.0|       1|\n",
      "|1002|  calculator|      55.0|       1|\n",
      "|2005|     A4paper|       7.8|       2|\n",
      "|2007|      eraser|       2.0|       4|\n",
      "|2001|watercolours|      12.5|       1|\n",
      "|2003|  paintbrush|       3.0|       4|\n",
      "+----+------------+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM sales\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b8141a-837f-4eb9-b4ac-ed1c3f4ebccf",
   "metadata": {
    "id": "76b8141a-837f-4eb9-b4ac-ed1c3f4ebccf",
    "outputId": "8938851c-aae9-4bf2-fb92-d80917804365"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, name: string, age: bigint, gender: string]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create DataFrame\n",
    "columns = [\"id\", \"name\", \"age\", \"gender\"]\n",
    "\n",
    "data = [(1, \"James\", 30, \"M\"),\n",
    "        (2, \"Ann\", 40, \"F\"),\n",
    "        (3, \"Jeff\",  41, \"M\"),\n",
    "        (4, \"Jennifer\", 20, \"F\")]\n",
    "\n",
    "employeeDF = spark.createDataFrame(data, columns)\n",
    "employeeDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b863c3-26e8-472a-95ae-8aecd14770ad",
   "metadata": {
    "id": "74b863c3-26e8-472a-95ae-8aecd14770ad",
    "outputId": "348d0a44-40cf-4a14-b433-3ff33e073b38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeeDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9955ea-572d-41db-bbd7-ee5a812cade0",
   "metadata": {
    "id": "0d9955ea-572d-41db-bbd7-ee5a812cade0",
    "outputId": "f5e69de4-7c29-4fa9-f4b4-a5bdd1c51e2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+------+\n",
      "| id|    name|age|gender|\n",
      "+---+--------+---+------+\n",
      "|  1|   James| 30|     M|\n",
      "|  2|     Ann| 40|     F|\n",
      "|  3|    Jeff| 41|     M|\n",
      "|  4|Jennifer| 20|     F|\n",
      "+---+--------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeeDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b665e0e2-455f-4785-a7d3-0f5a9b7eb50a",
   "metadata": {
    "id": "b665e0e2-455f-4785-a7d3-0f5a9b7eb50a",
    "outputId": "c0f3dbe0-3ec9-43bf-c9d3-979d75fa066b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+------+\n",
      "| id|    name|age|gender|\n",
      "+---+--------+---+------+\n",
      "|  1|   James| 30|     M|\n",
      "|  2|     Ann| 40|     F|\n",
      "|  3|    Jeff| 41|     M|\n",
      "|  4|Jennifer| 20|     F|\n",
      "+---+--------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create temporary view\n",
    "employeeDF.createOrReplaceTempView(\"emp_view\")\n",
    "spark.sql(\"SELECT * FROM emp_view\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd43927-39db-46f8-b4fc-11217b82e332",
   "metadata": {
    "id": "dbd43927-39db-46f8-b4fc-11217b82e332"
   },
   "source": [
    "## 3. Creating Databases and Tables\n",
    "\n",
    "### 3.1 Creating a Database\n",
    "Create a new database named **hrdb**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f1edbc-0e7a-4628-bb5f-c9c59342d4f6",
   "metadata": {
    "id": "95f1edbc-0e7a-4628-bb5f-c9c59342d4f6",
    "outputId": "4f4c317b-fa91-4de4-ec72-4e90e721fd0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "|     hrdb|\n",
      "|  salesdb|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DROP DATABASE IF EXISTS hrdb CASCADE\")\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS hrdb\")\n",
    "spark.sql(\"SHOW DATABASES\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953522cf-3b58-4046-b903-254e29d01b76",
   "metadata": {
    "id": "953522cf-3b58-4046-b903-254e29d01b76"
   },
   "source": [
    "### 3.2 Creating a Table\n",
    "Create a table named as **emp_table** in the **hrdb** database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9118b286-582a-439e-bf15-8376bb15fc68",
   "metadata": {
    "id": "9118b286-582a-439e-bf15-8376bb15fc68",
    "outputId": "583f1d22-a452-4ed7-b95d-b550c5fd0aca"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'salesdb'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check current database\n",
    "spark.catalog.currentDatabase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a68a7d-abff-4fc2-aafc-1a00a700363e",
   "metadata": {
    "id": "74a68a7d-abff-4fc2-aafc-1a00a700363e",
    "outputId": "4d0c77fc-f757-4f04-aafb-def12c5bb390"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hrdb'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Switch to the hrdb database\n",
    "spark.sql(\"USE hrdb\")\n",
    "spark.catalog.currentDatabase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3323f77f-d9c0-4fea-9aa6-f72672ba1494",
   "metadata": {
    "id": "3323f77f-d9c0-4fea-9aa6-f72672ba1494",
    "outputId": "ae9962f1-b47f-4ee4-face-06dbcfc7b42a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "|         | emp_view|       true|\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check current tables\n",
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907d70e9-9855-4571-9811-993795fb4406",
   "metadata": {
    "id": "907d70e9-9855-4571-9811-993795fb4406",
    "outputId": "047ddb3e-12d4-4001-e944-df1829d36011"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "|     hrdb|emp_table|      false|\n",
      "|         | emp_view|       true|\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/31 11:40:45 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "spark.sql(\"CREATE TABLE hrdb.emp_table (id INT, name STRING, age INT, gender STRING)\")\n",
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef1b870-aa28-4e3a-8eab-544b411a29df",
   "metadata": {
    "id": "cef1b870-aa28-4e3a-8eab-544b411a29df"
   },
   "source": [
    "## 4. Inserting Data into Tables\n",
    "Insert data from **emp_view** into **emp_table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32e9a61-f077-465a-888e-729c959ae804",
   "metadata": {
    "id": "e32e9a61-f077-465a-888e-729c959ae804",
    "outputId": "ded5925e-f93c-4032-bdc7-03845c676cee"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"INSERT INTO TABLE hrdb.emp_table  SELECT * FROM emp_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dd180d-792f-445e-a9c5-d17fbe44ba65",
   "metadata": {
    "id": "72dd180d-792f-445e-a9c5-d17fbe44ba65",
    "outputId": "d751f958-75b0-4374-d56c-c90aa840f525"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+------+\n",
      "| id|    name|age|gender|\n",
      "+---+--------+---+------+\n",
      "|  1|   James| 30|     M|\n",
      "|  2|     Ann| 40|     F|\n",
      "|  3|    Jeff| 41|     M|\n",
      "|  4|Jennifer| 20|     F|\n",
      "+---+--------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# View data from emp_table\n",
    "spark.sql(\"SELECT * FROM hrdb.emp_table\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbd24a9-4250-4e83-8d8f-7c90175c776c",
   "metadata": {
    "id": "fdbd24a9-4250-4e83-8d8f-7c90175c776c"
   },
   "source": [
    "## 5. Read Hive table using table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77794a8a-3eb1-45fd-88bf-bb266df33ad3",
   "metadata": {
    "id": "77794a8a-3eb1-45fd-88bf-bb266df33ad3",
    "outputId": "8f33e04c-3c0d-487d-bacd-821405817cc9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "|  salesdb|  invites|      false|\n",
      "|  salesdb|    pokes|      false|\n",
      "|  salesdb|    sales|      false|\n",
      "|         | emp_view|       true|\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"USE salesdb\")\n",
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209a4f07-160f-4720-ba4e-57f00ec1bae9",
   "metadata": {
    "id": "209a4f07-160f-4720-ba4e-57f00ec1bae9",
    "outputId": "06c9cf57-2ae2-4e7b-af05-1ef9df948574"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------+----------+--------+\n",
      "|  id| description|unit_price|quantity|\n",
      "+----+------------+----------+--------+\n",
      "|1005|         pen|       2.5|       4|\n",
      "|1007|      pencil|       1.0|      10|\n",
      "|1001|    notebook|       5.0|       2|\n",
      "|1003|       ruler|       1.0|       1|\n",
      "|1002|  calculator|      55.0|       1|\n",
      "|2005|     A4paper|       7.8|       2|\n",
      "|2007|      eraser|       2.0|       4|\n",
      "|2001|watercolours|      12.5|       1|\n",
      "|2003|  paintbrush|       3.0|       4|\n",
      "+----+------------+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.table(\"sales\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac97d2d-566d-4386-b123-77ba39141a8c",
   "metadata": {
    "id": "fac97d2d-566d-4386-b123-77ba39141a8c",
    "outputId": "8e3509b6-6e98-40b6-9d20-75dcacbaa4d3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hdfs:/user/hive/warehouse/'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf.get('spark.sql.warehouse.dir')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2ff320-49a4-4d36-83b2-0ecd256242a8",
   "metadata": {
    "id": "4a2ff320-49a4-4d36-83b2-0ecd256242a8"
   },
   "source": [
    "## 6. Dropping a Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0886a09c-d199-4d91-aa3f-c768ea03e8aa",
   "metadata": {
    "id": "0886a09c-d199-4d91-aa3f-c768ea03e8aa",
    "outputId": "e4af71df-85d1-41b8-8882-865f47a97cc9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "|     hrdb|\n",
      "|  salesdb|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DROP DATABASE IF EXISTS de_company CASCADE\")\n",
    "spark.sql(\"SHOW DATABASES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c0a1d1-ecd6-444f-b99b-8ec689f7b65b",
   "metadata": {
    "id": "a6c0a1d1-ecd6-444f-b99b-8ec689f7b65b"
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "de-venv",
   "language": "python",
   "name": "de-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
